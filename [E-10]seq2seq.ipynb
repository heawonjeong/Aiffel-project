{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a102d4dc",
   "metadata": {},
   "source": [
    "## 번역기 만들기 (1) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34b4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1383344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체샘플의 수 :  217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215002</th>\n",
       "      <td>No one knows exactly how many people considere...</td>\n",
       "      <td>Personne ne sait exactement combien de personn...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42561</th>\n",
       "      <td>Tom doesn't need it.</td>\n",
       "      <td>Tom n'en a pas besoin.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72213</th>\n",
       "      <td>I've solved the problem.</td>\n",
       "      <td>J'ai résolu le problème.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31123</th>\n",
       "      <td>Do you need a ride?</td>\n",
       "      <td>Avez-vous besoin que je vous conduise ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114791</th>\n",
       "      <td>I'm afraid we're out of time.</td>\n",
       "      <td>Je crains que nous ne soyons pas à l'heure.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "215002  No one knows exactly how many people considere...   \n",
       "42561                                Tom doesn't need it.   \n",
       "72213                            I've solved the problem.   \n",
       "31123                                 Do you need a ride?   \n",
       "114791                      I'm afraid we're out of time.   \n",
       "\n",
       "                                                      fra  \\\n",
       "215002  Personne ne sait exactement combien de personn...   \n",
       "42561                              Tom n'en a pas besoin.   \n",
       "72213                            J'ai résolu le problème.   \n",
       "31123             Avez-vous besoin que je vous conduise ?   \n",
       "114791        Je crains que nous ne soyons pas à l'heure.   \n",
       "\n",
       "                                                       cc  \n",
       "215002  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n",
       "42561   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "72213   CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "31123   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "114791  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+ '/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng','fra','cc'], sep='\\t')\n",
    "print('전체샘플의 수 : ', len(lines))\n",
    "lines.sample(5) # 샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417cc485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26480</th>\n",
       "      <td>I want to workout.</td>\n",
       "      <td>Je veux faire de l'exercice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47292</th>\n",
       "      <td>I think Tom is drunk.</td>\n",
       "      <td>Je pense que Tom est bourré.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43410</th>\n",
       "      <td>We're finally alone.</td>\n",
       "      <td>Nous sommes enfin seules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26306</th>\n",
       "      <td>I really like you.</td>\n",
       "      <td>Tu me plais vraiment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12938</th>\n",
       "      <td>Tom knows that.</td>\n",
       "      <td>Tom sait ça.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         eng                           fra\n",
       "26480     I want to workout.  Je veux faire de l'exercice.\n",
       "47292  I think Tom is drunk.  Je pense que Tom est bourré.\n",
       "43410   We're finally alone.     Nous sommes enfin seules.\n",
       "26306     I really like you.         Tu me plais vraiment.\n",
       "12938        Tom knows that.                  Tom sait ça."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#세번째 열 제거, 훈련데이터 5만개로 줄입니다.\n",
    "lines = lines[['eng', 'fra']][:50000]\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a377f83",
   "metadata": {},
   "source": [
    "seq2seq 동작을 위해 디코더의 입력과 예측에는 시작 토큰 <sos>와 종료토큰 <eos>를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af28fa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 :  50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29130</th>\n",
       "      <td>Tom seems excited.</td>\n",
       "      <td>\\t Tom semble emballé. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40085</th>\n",
       "      <td>I will not help you.</td>\n",
       "      <td>\\t Je ne t'aiderai pas. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45422</th>\n",
       "      <td>Drink a lot of water.</td>\n",
       "      <td>\\t Buvez beaucoup d'eau. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944</th>\n",
       "      <td>I followed you.</td>\n",
       "      <td>\\t Je t'ai suivi. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42260</th>\n",
       "      <td>They need the money.</td>\n",
       "      <td>\\t Elles ont besoin de l'argent. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         eng                                  fra\n",
       "29130     Tom seems excited.            \\t Tom semble emballé. \\n\n",
       "40085   I will not help you.           \\t Je ne t'aiderai pas. \\n\n",
       "45422  Drink a lot of water.          \\t Buvez beaucoup d'eau. \\n\n",
       "10944        I followed you.                 \\t Je t'ai suivi. \\n\n",
       "42260   They need the money.  \\t Elles ont besoin de l'argent. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t ' + x + ' \\n')\n",
    "print('전체 샘플의 수 : ', len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d322c58",
   "metadata": {},
   "source": [
    "단어장(vocabulary)을 만들고 각 단어에 부여된 고유한 정수로 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩 과정을 거칩니다.영어와 프랑스어는 단어장을 별도로 만들어줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b2af6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 7], [19, 4, 7], [19, 4, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True) # 문자단위로 Tokenizer 생성\n",
    "eng_tokenizer.fit_on_texts(lines.eng) # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng) # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12596c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 14, 1, 11],\n",
       " [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True) # 문자 단위로 Tokenizer 생성\n",
    "fra_tokenizer.fit_on_texts(lines.fra) # 50000개의 행을 가진 fra의 각 행에 토큰화 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra) # 단어를 숫자값 인덱스로 변환\n",
    "target_text[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be62275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 52\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "263c5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어시퀀스의 최대길이 21\n",
      "프랑스어 시퀀스의 최대길이 69\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어시퀀스의 최대길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997e936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 52\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca18bc",
   "metadata": {},
   "source": [
    "인코더의 입력으로 사용되는 영어 시퀀스와 달리, 프랑스어 시퀀스는 2가지 버전으로 나누어 준비합니다. 하나는 디코더의 출력과 비교해야 할 정답 데이터로 사용해야 할 원래 목적에 따른 것이고 다른 하나는 교사강요(Teacher forcing)을 위해 디코더의 입력으로 사용하기 위함입니다.\n",
    "디코더의 입력으로 사용할 시퀀스는 <eos> 토큰이 필요 없고, 디코더의 출력과 비교할 정답데이터로 사용해야할 시퀀스는 <sos> 토큰이 필요 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807f7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2f5df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1, 19, 5, 1, 31, 1], [10, 1, 15, 5, 12, 16, 29, 2, 14, 1], [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 11], [1, 15, 5, 12, 16, 29, 2, 14, 1, 11], [1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 입력과 출력을 print\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1f3ac",
   "metadata": {},
   "source": [
    "디코더 입력은 숫자 11(<eos>토큰)가 제거되었고, 디코더 출력은 숫자 10(<sos>토큰)이 제거 되었습니다.\n",
    "패딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0948fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f0fa1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  4  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# 인코더 샘플 하나 출력\n",
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf32f4",
   "metadata": {},
   "source": [
    "각 정수에 대해서 벡터화 방법으로 원-핫 인코딩을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a8f6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21, 52)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f30c33",
   "metadata": {},
   "source": [
    "데이터의 크기는 (샘플의수 * 샘플의 길이 * 단어장의 크기)\n",
    "원-핫 인코딩은 각 정수를 단어장의 크기를 가지는 원-핫 벡터로 인코딩 하는 과정입니다.\n",
    "\n",
    "훈련과정의 validation을 위해 위에서 생성한 데이터 50000건중 3000건만 validation set으로 잡고, 나머지는 학습데이터로 잡습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568f9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 21, 52)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 69, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 69, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]     ## ???? -n_of_val\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f84abe",
   "metadata": {},
   "source": [
    "## 번역기 만들기 (2) 모델 훈련하기\n",
    "\n",
    "*  A ten-minute introduction to sequence-to-sequence learning 을 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb5f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense \n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bf848",
   "metadata": {},
   "source": [
    "#### 인코더 설계\n",
    "* 인코더의 마지막 hidden state를 디코더의 첫번째 hidden state로 사용합니다. \n",
    "* 기본 RNN 보다 좀더 복잡한 LSTM의 경우 hidden state뿐만 아니라 cell state가 존재합니다.\n",
    "* 인코더 LSTM 셀의 마지막 time step의 hidden state와 cell state를 디코더 LSTM의 첫번째 hidden state와 cell state로 전달해 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6f3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성\n",
    "# LSTM의 입력텐서를 정의합니다. 입력 문장을 저장하게 될 변수 텐서.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "\n",
    "# 256의 hidden_size를 가지는 LSTM 셀 생성. LSTM의 수용력(capacity)\n",
    "# return_state = True를 해서 hidden state와 cell state를 리턴받을 수 있도록 합니다.\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "\n",
    "# 입력 텐서를 사용하여 마지막 time step의 hidden state와 cell state를 결과로 받습니다.\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder output은 여기서 불필요\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# hidden state 와 cell state를 다음 time step으로 전달하기 위해서 별도 저장\n",
    "# 마지막 time step의 hidden state와 cell state를 encoder_states 라는 하나의 변수에 저장.\n",
    "# 앞으로 이를 디코더에 전달한다.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec918fe",
   "metadata": {},
   "source": [
    "#### 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e61daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76c856",
   "metadata": {},
   "source": [
    "#### 디코더 출력층 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd296c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4efde30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 52)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 316416      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 673,097\n",
      "Trainable params: 673,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc636c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 49s 26ms/step - loss: 0.9511 - val_loss: 0.8141\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.5780 - val_loss: 0.6662\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.4804 - val_loss: 0.5785\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.4250 - val_loss: 0.5348\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3870 - val_loss: 0.5039\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3588 - val_loss: 0.4767\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3367 - val_loss: 0.4518\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3192 - val_loss: 0.4343\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3047 - val_loss: 0.4216\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2925 - val_loss: 0.4088\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2819 - val_loss: 0.4067\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2729 - val_loss: 0.3933\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2646 - val_loss: 0.3906\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2574 - val_loss: 0.3909\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2505 - val_loss: 0.3876\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2444 - val_loss: 0.3841\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2387 - val_loss: 0.3808\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2333 - val_loss: 0.3811\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2285 - val_loss: 0.3770\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2237 - val_loss: 0.3775\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2193 - val_loss: 0.3736\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2150 - val_loss: 0.3763\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2110 - val_loss: 0.3716\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2071 - val_loss: 0.3745\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2033 - val_loss: 0.3727\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1999 - val_loss: 0.3738\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1967 - val_loss: 0.3779\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1934 - val_loss: 0.3758\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1904 - val_loss: 0.3765\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1874 - val_loss: 0.3779\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1844 - val_loss: 0.3815\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1816 - val_loss: 0.3828\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1791 - val_loss: 0.3846\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1765 - val_loss: 0.3858\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1740 - val_loss: 0.3876\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1715 - val_loss: 0.3935\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1691 - val_loss: 0.3906\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1670 - val_loss: 0.3937\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1645 - val_loss: 0.3953\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1625 - val_loss: 0.3979\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1604 - val_loss: 0.4003\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1583 - val_loss: 0.4112\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1564 - val_loss: 0.4075\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1545 - val_loss: 0.4080\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1526 - val_loss: 0.4083\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1509 - val_loss: 0.4123\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1491 - val_loss: 0.4132\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1474 - val_loss: 0.4172\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1457 - val_loss: 0.4158\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1440 - val_loss: 0.4168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2863a8d490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n",
    "         validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "         batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbcf24",
   "metadata": {},
   "source": [
    "## 번역기 만들기 (3) 모델 테스트 하기\n",
    "\n",
    "#### 테스트 단계의 디코더 동작 순서\n",
    "1. 인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state를 얻는다.\n",
    "2. <sos> 토큰인 '\\t'를 디코더에 입력한다.\n",
    "3. 이전 time step의 출력층의 예측결과를 현재 time step의 입력으로 한다.\n",
    "4. 3을 반복하다가 <eos> 토큰인 '\\n'이 예측되면 이를 중단한다. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1abb513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 52)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 316416    \n",
      "=================================================================\n",
      "Total params: 316,416\n",
      "Trainable params: 316,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더 정의(이미정의한 것 재사용)\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d6954ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "\n",
    "# 이전 time step의 hidden layer와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence() 에 구현\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "\n",
    "\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc17f13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층을 재설계\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "536867df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d09f7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67580b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:  partez. \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장:  Bonjour ! \n",
      "번역기가 번역한 문장:  salut. \n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장:  Compris ! \n",
      "번역기가 번역한 문장:  continue ! \n",
      "-----------------------------------\n",
      "입력 문장: Goodbye.\n",
      "정답 문장:  Au revoir. \n",
      "번역기가 번역한 문장:  cieuss ! \n",
      "-----------------------------------\n",
      "입력 문장: Hands off.\n",
      "정답 문장:  Pas touche ! \n",
      "번역기가 번역한 문장:  tenez votre casse ! \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbc59d",
   "metadata": {},
   "source": [
    "# 프로젝트 : 단어 Level로 번역기 업그레이드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfde4f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118cb6d",
   "metadata": {},
   "source": [
    "동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 to_categorical()함수가 아닌 임베딩 층을 추가하여 단어 단위의 번역기를 완성해봅니다. 데이터에서 상위 33,000개의 샘플만 사용합니다. 33000ro wnd 3000개는 테스트 데이터로 분리하여 모델을 학습한 후 번역을 테스트한 용도로 사용합니다.\n",
    "\n",
    "## step 1. 정제, 정규화, 전처리(영어, 프랑스어)\n",
    "\n",
    "1. 구두점(punctuation)을 단어와 분리\n",
    "2. 소문자로\n",
    "3. 띄어쓰기 단위로 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a11e1498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25640</th>\n",
       "      <td>I checked my bags.</td>\n",
       "      <td>\\t J'ai enregistré mes bagages. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>Am I stupid?</td>\n",
       "      <td>\\t Suis-je idiote ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>Is it safe?</td>\n",
       "      <td>\\t Est-ce inoffensif ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>I am on duty now.</td>\n",
       "      <td>\\t Maintenant je suis en service. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17209</th>\n",
       "      <td>Tom followed me.</td>\n",
       "      <td>\\t Tom me suivait. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                                   fra\n",
       "25640  I checked my bags.    \\t J'ai enregistré mes bagages. \\n\n",
       "2769         Am I stupid?                \\t Suis-je idiote ? \\n\n",
       "2232          Is it safe?             \\t Est-ce inoffensif ? \\n\n",
       "19705   I am on duty now.  \\t Maintenant je suis en service. \\n\n",
       "17209    Tom followed me.                 \\t Tom me suivait. \\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 33000개의 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5a6952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2db3e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng 전처리\n",
    "lines.eng = lines.eng.apply(lambda x : preprocess_sentence(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c7ab5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23568       [what, s, your, beef, ?]\n",
       "14886         [i, felt, betrayed, .]\n",
       "18978       [don, t, be, alarmed, .]\n",
       "31752    [he, sat, next, to, her, .]\n",
       "31860    [he, s, working, on, it, .]\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9dd545",
   "metadata": {},
   "source": [
    "## step 2. 디코더의 문장에 시작토큰과 종료토큰 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f70c3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 후 디코더\n",
    "def preprocess_sentence_decoder(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8b48e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.fra = lines.fra.apply(lambda x : preprocess_sentence_decoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "342baedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25723    [<sos>, je, ne, dispose, pas, de, beaucoup, de...\n",
       "16417          [<sos>, personne, ne, l, a, fait, ., <eos>]\n",
       "378                          [<sos>, moi, aussi, ., <eos>]\n",
       "13413               [<sos>, quel, chien, massif, !, <eos>]\n",
       "12958      [<sos>, tom, a, l, air, d, tre, mort, ., <eos>]\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d21b8e",
   "metadata": {},
   "source": [
    "## step 3. 케라스의 토크나이저로 텍스트 숫자로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6ee849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어와 프랑스어에 대한 토크나이저 생성, tokenizer.texts_to_sequences()함수를 사용\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(lines.eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5543fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'i': 2, 'you': 3, '?': 4, 'tom': 5, 'it': 6, 's': 7, 'is': 8, 'a': 9, 're': 10, 'we': 11, 't': 12, 'm': 13, 'that': 14, 'he': 15, 'the': 16, 'me': 17, 'this': 18, 'to': 19, 'was': 20, 'are': 21, 'do': 22, '!': 23, 'can': 24, 'they': 25, 'have': 26, 'go': 27, 'don': 28, 'my': 29, 'your': 30, 'be': 31, 'll': 32, 'not': 33, 'what': 34, 'did': 35, 'here': 36, 'no': 37, 'like': 38, 'all': 39, 'let': 40, 'she': 41, 'how': 42, 'in': 43, 'get': 44, 'up': 45, 'need': 46, 'on': 47, 'very': 48, 'want': 49, 'love': 50, 'one': 51, 'who': 52, 'know': 53, 'him': 54, 'come': 55, 'out': 56, 'got': 57, 'please': 58, 'just': 59, 'look': 60, 'now': 61, 'so': 62, 'help': 63, 'there': 64, 'us': 65, 'too': 66, 'stop': 67, 'take': 68, 'of': 69, 'see': 70, 'good': 71, 'will': 72, 'were': 73, 'for': 74, 've': 75, 'am': 76, 'has': 77, 'had': 78, 'at': 79, 'give': 80, 'must': 81, 'where': 82, 'saw': 83, 'keep': 84, 'feel': 85, 'back': 86, 'try': 87, 'home': 88, 'work': 89, 'alone': 90, 'stay': 91, 'didn': 92, 'leave': 93, 'happy': 94, 'why': 95, 'isn': 96, 'hate': 97, 'everyone': 98, 'again': 99, 'lost': 100, 'her': 101, 'them': 102, 'car': 103, 'won': 104, 'busy': 105, 'eat': 106, 'an': 107, 'made': 108, 'his': 109, 'time': 110, 'felt': 111, 'may': 112, 'right': 113, 'went': 114, 'never': 115, 'dog': 116, 'well': 117, 'down': 118, 'wait': 119, 'job': 120, 'with': 121, 'does': 122, 'away': 123, 'ready': 124, 'going': 125, 'mary': 126, 'still': 127, 'nice': 128, 'done': 129, 'bad': 130, 'big': 131, 'looks': 132, 'tell': 133, 'think': 134, 'say': 135, 'drink': 136, 'late': 137, 'make': 138, 'fun': 139, 'could': 140, 'mine': 141, 'hurt': 142, 'about': 143, 'over': 144, 'tired': 145, 'ok': 146, 'off': 147, 'money': 148, 'left': 149, 'call': 150, 'should': 151, 'book': 152, 'some': 153, 'talk': 154, 'safe': 155, 'read': 156, 'kept': 157, 'careful': 158, 'came': 159, 'lot': 160, 'wrong': 161, 'likes': 162, 'french': 163, 'trust': 164, 'old': 165, 'open': 166, 'wasn': 167, 'really': 168, 'new': 169, 'live': 170, 'funny': 171, 'our': 172, 'sit': 173, 'enough': 174, 'hurry': 175, 'run': 176, 'cold': 177, 'died': 178, 'more': 179, 'sure': 180, 'bed': 181, 'day': 182, 'room': 183, 'said': 184, 'easy': 185, 'much': 186, 'way': 187, 'ask': 188, 'd': 189, 'hope': 190, 'broke': 191, 'idea': 192, 'find': 193, 'loves': 194, 'hard': 195, 'hear': 196, 'nobody': 197, 'miss': 198, 'today': 199, 'gave': 200, 'everybody': 201, 'crazy': 202, 'upset': 203, 'watch': 204, 'both': 205, 'everything': 206, 'sad': 207, 'man': 208, 'found': 209, 'cat': 210, 'seems': 211, 'win': 212, 'aren': 213, 'lucky': 214, 'water': 215, 'sleep': 216, 'sick': 217, 'die': 218, 'yours': 219, 'life': 220, 'by': 221, 'door': 222, 'knew': 223, 'forget': 224, 'mad': 225, 'anyone': 226, 'better': 227, 'pay': 228, 'bit': 229, 'nothing': 230, 'play': 231, 'quit': 232, 'free': 233, 'early': 234, 'called': 235, 'buy': 236, 'care': 237, 'angry': 238, 'turn': 239, 'bring': 240, 'fine': 241, 'swim': 242, 'walk': 243, 'dead': 244, 'true': 245, 'these': 246, 'gone': 247, 'stupid': 248, 'friends': 249, 'house': 250, 'sing': 251, 'put': 252, 'and': 253, 'name': 254, 'heard': 255, 'great': 256, 'school': 257, 'check': 258, 'eyes': 259, 'hand': 260, 'cool': 261, 'fast': 262, 'mean': 263, 'close': 264, 'kidding': 265, 'took': 266, 'looked': 267, 'tall': 268, 'shot': 269, 'sat': 270, 'hungry': 271, 'beer': 272, 'working': 273, 'fat': 274, 'move': 275, 'perfect': 276, 'stand': 277, 'drunk': 278, 'first': 279, 'knows': 280, 'wine': 281, 'needed': 282, 'speak': 283, 'quiet': 284, 'serious': 285, 'enjoy': 286, 'best': 287, 'doing': 288, 'calm': 289, 'show': 290, 'answer': 291, 'coming': 292, 'yourself': 293, 'ate': 294, 'hold': 295, 'cry': 296, 'liked': 297, 'remember': 298, 'anybody': 299, 'only': 300, 'scared': 301, 'yet': 302, 'sorry': 303, 'outside': 304, 'loved': 305, 'boy': 306, 'hat': 307, 'key': 308, 'running': 309, 'been': 310, 'from': 311, 'fix': 312, 'ahead': 313, 'start': 314, 'son': 315, 'friend': 316, 'hair': 317, 'thanks': 318, 'fired': 319, 'inside': 320, 'hot': 321, 'bus': 322, 'naive': 323, 'pretty': 324, 'tried': 325, 'wake': 326, 'brave': 327, 'use': 328, 'weird': 329, 'food': 330, 'nuts': 331, 'stopped': 332, 'wants': 333, 'believe': 334, 'myself': 335, 'seem': 336, 'already': 337, 'fell': 338, 'touch': 339, 'follow': 340, 'once': 341, 'fish': 342, 'coffee': 343, 'owe': 344, 'began': 345, 'handle': 346, 'agree': 347, 'drive': 348, 'lie': 349, 'long': 350, 'write': 351, 'rest': 352, 'problem': 353, 'lose': 354, 'plan': 355, 'when': 356, 'beat': 357, 'sign': 358, 'met': 359, 'almost': 360, 'bag': 361, 'warned': 362, 'finished': 363, 'wanted': 364, 'doesn': 365, 'shut': 366, 'ran': 367, 'works': 368, 'clean': 369, 'told': 370, 'sweet': 371, 'nervous': 372, 'joke': 373, 'hates': 374, 'around': 375, 'young': 376, 'change': 377, 'married': 378, 'girls': 379, 'someone': 380, 'listen': 381, 'kind': 382, 'drop': 383, 'catch': 384, 'lying': 385, 'break': 386, 'changed': 387, 'needs': 388, 'kids': 389, 'laughed': 390, 'gun': 391, 'music': 392, 'those': 393, 'lazy': 394, 'rich': 395, 'hurts': 396, 'red': 397, 'soon': 398, 'tv': 399, 'missed': 400, 'seat': 401, 'caught': 402, 'talking': 403, 'quickly': 404, 'boston': 405, 'wife': 406, 'as': 407, 'word': 408, 'somebody': 409, 'grab': 410, 'two': 411, 'lunch': 412, 'whose': 413, 'invited': 414, 'pen': 415, 'quite': 416, 'lied': 417, 'awesome': 418, 'cute': 419, 'forgot': 420, 'luck': 421, 'wonderful': 422, 'child': 423, 'father': 424, 'boss': 425, 'arrived': 426, 'couldn': 427, 'smoke': 428, 'game': 429, 'after': 430, 'strong': 431, 'waited': 432, 'tea': 433, 'normal': 434, 'prepared': 435, 'often': 436, 'excited': 437, 'crying': 438, 'doctor': 439, 'would': 440, 'oldest': 441, 'shy': 442, 'dogs': 443, 'tough': 444, 'smell': 445, 'smart': 446, 'deal': 447, 'asked': 448, 'slept': 449, 'seen': 450, 'something': 451, 'trusted': 452, 'tomorrow': 453, 'being': 454, 'sounds': 455, 'cried': 456, 'lock': 457, 'spoke': 458, 'awful': 459, 'cut': 460, 'joking': 461, 'lonely': 462, 'study': 463, 'fight': 464, 'milk': 465, 'saved': 466, 'afraid': 467, 'mom': 468, 'teacher': 469, 'phone': 470, 'paid': 471, 'fair': 472, 'hit': 473, 'kill': 474, 'rude': 475, 'helped': 476, 'weak': 477, 'worked': 478, 'guess': 479, 'confused': 480, 'happen': 481, 'amazing': 482, 'killed': 483, 'dinner': 484, 'horse': 485, 'important': 486, 'tie': 487, 'bike': 488, 'terrific': 489, 'timid': 490, 'walked': 491, 'moving': 492, 'next': 493, 'bought': 494, 'winning': 495, 'worried': 496, 'pain': 497, 'arm': 498, 'always': 499, 'which': 500, 'betrayed': 501, 'guy': 502, 'if': 503, 'sound': 504, 'hang': 505, 'guys': 506, 'honest': 507, 'cheated': 508, 'retired': 509, 'alive': 510, 'dark': 511, 'sleepy': 512, 'ignore': 513, 'vote': 514, 'count': 515, 'laugh': 516, 'started': 517, 'slowly': 518, 'upstairs': 519, 'might': 520, 'singing': 521, 'own': 522, 'minute': 523, 'choice': 524, 'mother': 525, 'place': 526, 'relax': 527, 'slow': 528, 'fit': 529, 'wise': 530, 'smiled': 531, 'skinny': 532, 'town': 533, 'drank': 534, 'leaving': 535, 'shoes': 536, 'blame': 537, 'dream': 538, 'became': 539, 'light': 540, 'into': 541, 'head': 542, 'relaxed': 543, 'age': 544, 'cats': 545, 'reading': 546, 'song': 547, 'send': 548, 'books': 549, 'smiling': 550, 'student': 551, 'fault': 552, 'fire': 553, 'hello': 554, 'kiss': 555, 'full': 556, 'excuse': 557, 'woke': 558, 'cook': 559, 'dying': 560, 'short': 561, 'step': 562, 'hired': 563, 'duty': 564, 'last': 565, 'eating': 566, 'trying': 567, 'news': 568, 'many': 569, 'mess': 570, 'things': 571, 'sent': 572, 'beautiful': 573, 'surprised': 574, 'end': 575, 'turned': 576, 'trip': 577, 'join': 578, 'real': 579, 'warm': 580, 'yes': 581, 'mind': 582, 'bored': 583, 'correct': 584, 'unlucky': 585, 'secret': 586, 'bite': 587, 'empty': 588, 'boring': 589, 'family': 590, 'used': 591, 'every': 592, 'getting': 593, 'wet': 594, 'push': 595, 'save': 596, 'hands': 597, 'obey': 598, 'blind': 599, 'far': 600, 'tight': 601, 'face': 602, 'liar': 603, 'asleep': 604, 'ashamed': 605, 'fool': 606, 'law': 607, 'trapped': 608, 'silent': 609, 'kid': 610, 'heart': 611, 'pizza': 612, 'understand': 613, 'small': 614, 'coat': 615, 'foolish': 616, 'continue': 617, 'failed': 618, 'choose': 619, 'pardon': 620, 'thank': 621, 'famous': 622, 'cash': 623, 'bet': 624, 'set': 625, 'driving': 626, 'fake': 627, 'kissed': 628, 'meet': 629, 'another': 630, 'innocent': 631, 'cruel': 632, 'cops': 633, 'guilty': 634, 'god': 635, 'japanese': 636, 'exhausted': 637, 'throw': 638, 'point': 639, 'keys': 640, 'behind': 641, 'bill': 642, 'trouble': 643, 'story': 644, 'jump': 645, 'ill': 646, 'refuse': 647, 'ugly': 648, 'seated': 649, 'fly': 650, 'ours': 651, 'patient': 652, 'worry': 653, 'blue': 654, 'creative': 655, 'finish': 656, 'curious': 657, 'strange': 658, 'happened': 659, 'party': 660, 'list': 661, 'shall': 662, 'refused': 663, 'meat': 664, 'explain': 665, 'involved': 666, 'meant': 667, 'together': 668, 'terrible': 669, 'ball': 670, 'lives': 671, 'watching': 672, 'dangerous': 673, 'truth': 674, 'doors': 675, 'desk': 676, 'smile': 677, 'hug': 678, 'stood': 679, 'wash': 680, 'welcome': 681, 'helps': 682, 'armed': 683, 'awake': 684, 'men': 685, 'forgive': 686, 'girl': 687, 'hero': 688, 'pick': 689, 'war': 690, 'jealous': 691, 'shocked': 692, 'thirsty': 693, 'useless': 694, 'closer': 695, 'wish': 696, 'anything': 697, 'hey': 698, 'silly': 699, 'map': 700, 'nearby': 701, 'feet': 702, 'polite': 703, 'matter': 704, 'ride': 705, 'raise': 706, 'pleased': 707, 'danger': 708, 'listening': 709, 'satisfied': 710, 'carefully': 711, 'side': 712, 'movie': 713, 'opened': 714, 'talked': 715, 'taste': 716, 'warn': 717, 'naked': 718, 'agreed': 719, 'idiot': 720, 'clear': 721, 'along': 722, 'night': 723, 'sharp': 724, 'merciful': 725, 'box': 726, 'hated': 727, 'dance': 728, 'cake': 729, 'dancing': 730, 'broken': 731, 'escaped': 732, 'deserve': 733, 'proof': 734, 'risk': 735, 'shopping': 736, 'control': 737, 'survive': 738, 'people': 739, 'ambitious': 740, 'motivated': 741, 'boat': 742, 'trusts': 743, 'feeling': 744, 'hi': 745, 'stayed': 746, 'bald': 747, 'odd': 748, 'wrote': 749, 'drives': 750, 'fear': 751, 'stunned': 752, 'touched': 753, 'rain': 754, 'date': 755, 'team': 756, 'dress': 757, 'positive': 758, 'decided': 759, 'black': 760, 'yelling': 761, 'dreaming': 762, 'deny': 763, 'dry': 764, 'tricked': 765, 'nearly': 766, 'week': 767, 'resilient': 768, 'order': 769, 'window': 770, 'bell': 771, 'laughing': 772, 'insane': 773, 'camera': 774, 'rid': 775, 'park': 776, 'apple': 777, 'brought': 778, 'train': 779, 'goal': 780, 'sister': 781, 'begin': 782, 'high': 783, 'cover': 784, 'okay': 785, 'beg': 786, 'giggled': 787, 'content': 788, 'rush': 789, 'clever': 790, 'shoot': 791, 'losing': 792, 'punctual': 793, 'scream': 794, 'starved': 795, 'loud': 796, 'simple': 797, 'remain': 798, 'bath': 799, 'horrible': 800, 'pathetic': 801, 'maybe': 802, 'lawyer': 803, 'fearless': 804, 'shame': 805, 'looking': 806, 'writing': 807, 'followed': 808, 'unbelievable': 809, 'jobs': 810, 'favor': 811, 'downstairs': 812, 'comes': 813, 'or': 814, 'horses': 815, 'later': 816, 'near': 817, 'seemed': 818, 'rules': 819, 'lit': 820, 'knife': 821, 'having': 822, 'mistake': 823, 'sore': 824, 'baby': 825, 'ordered': 826, 'bicycle': 827, 'speaks': 828, 'shirt': 829, 'productive': 830, 'attack': 831, 'deaf': 832, 'prove': 833, 'birds': 834, 'dizzy': 835, 'stuck': 836, 'loosen': 837, 'amused': 838, 'direct': 839, 'drinks': 840, 'even': 841, 'bother': 842, 'decide': 843, 'discreet': 844, 'dumb': 845, 'baffled': 846, 'healthy': 847, 'sincere': 848, 'unhappy': 849, 'confident': 850, 'teach': 851, 'resist': 852, 'annoying': 853, 'powerful': 854, 'focused': 855, 'any': 856, 'smells': 857, 'studying': 858, 'believed': 859, 'fed': 860, 'finally': 861, 'fever': 862, 'prefer': 863, 'depressed': 864, 'different': 865, 'impressed': 866, 'sense': 867, 'little': 868, 'moment': 869, 'cup': 870, 'second': 871, 'yourselves': 872, 'disgusting': 873, 'travel': 874, 'rather': 875, 'escape': 876, 'store': 877, 'brother': 878, 'children': 879, 'noise': 880, 'fate': 881, 'flight': 882, 'person': 883, 'hide': 884, 'pass': 885, 'pull': 886, 'glad': 887, 'fantastic': 888, 'faster': 889, 'dirty': 890, 'burned': 891, 'promised': 892, 'single': 893, 'happens': 894, 'boys': 895, 'friendly': 896, 'grew': 897, 'chose': 898, 'fined': 899, 'sell': 900, 'moved': 901, 'cooking': 902, 'snow': 903, 'hugged': 904, 'objective': 905, 'dumped': 906, 'robbed': 907, 'singer': 908, 'fighting': 909, 'mistaken': 910, 'stubborn': 911, 'cancer': 912, 'special': 913, 'ended': 914, 'arguing': 915, 'tense': 916, 'nose': 917, 'badly': 918, 'soccer': 919, 'skiing': 920, 'rescued': 921, 'concerned': 922, 'forgetful': 923, 'screaming': 924, 'table': 925, 'six': 926, 'fishing': 927, 'makes': 928, 'plays': 929, 'excused': 930, 'message': 931, 'tonight': 932, 'proud': 933, 'low': 934, 'deep': 935, 'ski': 936, 'cheered': 937, 'promise': 938, 'stinks': 939, 'hers': 940, 'then': 941, 'breathe': 942, 'facts': 943, 'lovely': 944, 'built': 945, 'sneaky': 946, 'thirty': 947, 'matters': 948, 'cheap': 949, 'dressed': 950, 'also': 951, 'soup': 952, 'messed': 953, 'eggs': 954, 'manage': 955, 'certain': 956, 'staying': 957, 'memorize': 958, 'grow': 959, 'watched': 960, 'twins': 961, 'half': 962, 'plans': 963, 'cab': 964, 'paint': 965, 'divorced': 966, 'hopeless': 967, 'saint': 968, 'diary': 969, 'walking': 970, 'quietly': 971, 'sang': 972, 'woman': 973, 'crime': 974, 'gear': 975, 'few': 976, 'baked': 977, 'part': 978, 'dad': 979, 'summer': 980, 'advice': 981, 'played': 982, 'surrendered': 983, 'wore': 984, 'allow': 985, 'available': 986, 'powerless': 987, 'gets': 988, 'leak': 989, 'number': 990, 'chance': 991, 'tastes': 992, 'shoe': 993, 'injured': 994, 'crafty': 995, 'charge': 996, 'panting': 997, 'animals': 998, 'chair': 999, 'interested': 1000, 'reply': 1001, 'charming': 1002, 'stalling': 1003, 'students': 1004, 'same': 1005, 'envious': 1006, 'afford': 1007, 'hour': 1008, 'haven': 1009, 'bank': 1010, 'truly': 1011, 'sale': 1012, 'fabulous': 1013, 'himself': 1014, 'headache': 1015, 'frightened': 1016, 'but': 1017, 'thing': 1018, 'oh': 1019, 'goodbye': 1020, 'brief': 1021, 'phoned': 1022, 'crashed': 1023, 'hurried': 1024, 'noticed': 1025, 'fussy': 1026, 'seriously': 1027, 'prudent': 1028, 'quick': 1029, 'poor': 1030, 'doubt': 1031, 'air': 1032, 'panicked': 1033, 'screamed': 1034, 'loaded': 1035, 'aside': 1036, 'cheerful': 1037, 'ruthless': 1038, 'apologize': 1039, 'golf': 1040, 'gas': 1041, 'psychic': 1042, 'wealthy': 1043, 'monday': 1044, 'talks': 1045, 'coughed': 1046, 'says': 1047, 'admire': 1048, 'sugar': 1049, 'fan': 1050, 'learn': 1051, 'canadian': 1052, 'ears': 1053, 'sleeping': 1054, 'speaking': 1055, 'white': 1056, 'missing': 1057, 'closely': 1058, 'while': 1059, 'smoking': 1060, 'stoned': 1061, 'else': 1062, 'bird': 1063, 'demented': 1064, 'line': 1065, 'note': 1066, 'copy': 1067, 'volunteered': 1068, 'artist': 1069, 'impatient': 1070, 'treat': 1071, 'possible': 1072, 'peace': 1073, 'suit': 1074, 'breath': 1075, 'suits': 1076, 'scares': 1077, 'cousins': 1078, 'parents': 1079, 'complain': 1080, 'eaten': 1081, 'spot': 1082, 'cookies': 1083, 'harm': 1084, 'overworked': 1085, 'expensive': 1086, 'alike': 1087, 'death': 1088, 'relieved': 1089, 'match': 1090, 'born': 1091, 'alarmed': 1092, 'enemy': 1093, 'office': 1094, 'protect': 1095, 'fortunate': 1096, 'leader': 1097, 'knees': 1098, 'bothering': 1099, 'burn': 1100, 'thin': 1101, 'bless': 1102, 'fainted': 1103, 'voted': 1104, 'cares': 1105, 'destroy': 1106, 'fill': 1107, 'canceled': 1108, 'envy': 1109, 'fixed': 1110, 'flinched': 1111, 'accept': 1112, 'ruined': 1113, 'shaken': 1114, 'strict': 1115, 'green': 1116, 'generous': 1117, 'thorough': 1118, 'class': 1119, 'disagreed': 1120, 'bread': 1121, 'struggled': 1122, 'attend': 1123, 'finicky': 1124, 'doll': 1125, 'spring': 1126, 'surprise': 1127, 'act': 1128, 'dope': 1129, 'kicked': 1130, 'arrogant': 1131, 'romantic': 1132, 'apples': 1133, 'doubts': 1134, 'thought': 1135, 'twice': 1136, 'sue': 1137, 'offended': 1138, 'pregnant': 1139, 'checked': 1140, 'digging': 1141, 'gold': 1142, 'pale': 1143, 'uneasy': 1144, 'talented': 1145, 'beware': 1146, 'feed': 1147, 'clock': 1148, 'feels': 1149, 'deserved': 1150, 'winter': 1151, 'taxi': 1152, 'outrank': 1153, 'blood': 1154, 'convinced': 1155, 'delighted': 1156, 'intrigued': 1157, 'weapon': 1158, 'worth': 1159, 'foot': 1160, 'shower': 1161, 'tree': 1162, 'fooled': 1163, 'floor': 1164, 'doctors': 1165, 'enemies': 1166, 'nonsense': 1167, 'cows': 1168, 'scolded': 1169, 'attacked': 1170, 'wear': 1171, 'glasses': 1172, 'astonished': 1173, 'contagious': 1174, 'vegetarian': 1175, 'christmas': 1176, 'power': 1177, 'roses': 1178, 'picture': 1179, 'escaping': 1180, 'congratulations': 1181, 'glass': 1182, 'appreciate': 1183, 'pencil': 1184, 'kidnapped': 1185, 'wouldn': 1186, 'such': 1187, 'fresh': 1188, 'incredible': 1189, 'smaller': 1190, 'mouth': 1191, 'answered': 1192, 'straight': 1193, 'forgiven': 1194, 'flexible': 1195, 'yesterday': 1196, 'candle': 1197, 'overreacting': 1198, 'conceited': 1199, 'candles': 1200, 'weapons': 1201, 'courageous': 1202, 'master': 1203, 'banged': 1204, 'runs': 1205, 'drove': 1206, 'obeyed': 1207, 'rested': 1208, 'snowed': 1209, 'course': 1210, 'walks': 1211, 'admit': 1212, 'chuckled': 1213, 'disagree': 1214, 'exercise': 1215, 'greedy': 1216, 'humble': 1217, 'safer': 1218, 'comment': 1219, 'nap': 1220, 'notes': 1221, 'cheat': 1222, 'yelled': 1223, 'cancel': 1224, 'argue': 1225, 'jerk': 1226, 'rice': 1227, 'chicken': 1228, 'selfish': 1229, 'smashed': 1230, 'teasing': 1231, 'through': 1232, 'locked': 1233, 'split': 1234, 'leg': 1235, 'drowned': 1236, 'guessed': 1237, 'evil': 1238, 'fall': 1239, 'denied': 1240, 'exciting': 1241, 'apologized': 1242, 'draw': 1243, 'chess': 1244, 'jokes': 1245, 'star': 1246, 'genius': 1247, 'bluffing': 1248, 'expected': 1249, 'grateful': 1250, 'unfair': 1251, 'waste': 1252, 'obvious': 1253, 'command': 1254, 'scary': 1255, 'stole': 1256, 'translate': 1257, 'closed': 1258, 'annoy': 1259, 'flowers': 1260, 'english': 1261, 'thief': 1262, 'adorable': 1263, 'despise': 1264, 'tempted': 1265, 'terrified': 1266, 'raining': 1267, 'standing': 1268, 'bragging': 1269, 'adores': 1270, 'misled': 1271, 'remembers': 1272, 'packing': 1273, 'mail': 1274, 'bright': 1275, 'add': 1276, 'year': 1277, 'disappeared': 1278, 'outgoing': 1279, 'slapped': 1280, 'spiders': 1281, 'truck': 1282, 'painted': 1283, 'speechless': 1284, 'large': 1285, 'price': 1286, 'blew': 1287, 'their': 1288, 'helpful': 1289, 'similar': 1290, 'remembered': 1291, 'earlier': 1292, 'cleaned': 1293, 'expect': 1294, 'towel': 1295, 'hardly': 1296, 'partner': 1297, 'tongue': 1298, 'sons': 1299, 'swimming': 1300, 'tissue': 1301, 'rang': 1302, 'volunteer': 1303, 'sounded': 1304, 'city': 1305, 'its': 1306, 'ridiculous': 1307, 'decision': 1308, 'gray': 1309, 'repeat': 1310, 'believes': 1311, 'separated': 1312, 'daughter': 1313, 'knee': 1314, 'eye': 1315, 'threatened': 1316, 'cousin': 1317, 'taking': 1318, 'ever': 1319, 'showed': 1320, 'hilarious': 1321, 'unique': 1322, 'surrounded': 1323, 'talkative': 1324, 'three': 1325, 'example': 1326, 'rained': 1327, 'fold': 1328, 'stink': 1329, 'pack': 1330, 'cop': 1331, 'cooks': 1332, 'unlock': 1333, 'hung': 1334, 'north': 1335, 'ice': 1336, 'survived': 1337, 'twin': 1338, 'buying': 1339, 'pray': 1340, 'alert': 1341, 'cooked': 1342, 'harder': 1343, 'sec': 1344, 'fail': 1345, 'panic': 1346, 'eight': 1347, 'resigned': 1348, 'share': 1349, 'fruit': 1350, 'washed': 1351, 'dieting': 1352, 'engaged': 1353, 'popular': 1354, 'legal': 1355, 'flat': 1356, 'gift': 1357, 'cloudy': 1358, 'paper': 1359, 'return': 1360, 'huge': 1361, 'numb': 1362, 'guest': 1363, 'rifle': 1364, 'snack': 1365, 'easily': 1366, 'games': 1367, 'coward': 1368, 'farmer': 1369, 'addicted': 1370, 'faithful': 1371, 'obedient': 1372, 'outraged': 1373, 'reliable': 1374, 'reserved': 1375, 'starving': 1376, 'thrilled': 1377, 'hideous': 1378, 'goes': 1379, 'legs': 1380, 'recess': 1381, 'dreams': 1382, 'approved': 1383, 'pig': 1384, 'focus': 1385, 'overslept': 1386, 'loser': 1387, 'words': 1388, 'expert': 1389, 'pitch': 1390, 'maid': 1391, 'ignored': 1392, 'type': 1393, 'rope': 1394, 'fought': 1395, 'clue': 1396, 'ought': 1397, 'tied': 1398, 'pool': 1399, 'stabbed': 1400, 'disgusted': 1401, 'impulsive': 1402, 'uninsured': 1403, 'apart': 1404, 'unlocked': 1405, 'climbing': 1406, 'proceed': 1407, 'attention': 1408, 'clearly': 1409, 'stuff': 1410, 'anyway': 1411, 'creepy': 1412, 'vulgar': 1413, 'remarried': 1414, 'till': 1415, 'succeed': 1416, 'related': 1417, 'waiting': 1418, 'calling': 1419, 'grumpy': 1420, 'form': 1421, 'van': 1422, 'wide': 1423, 'arrested': 1424, 'playing': 1425, 'sun': 1426, 'captured': 1427, 'deceived': 1428, 'welcomed': 1429, 'devastated': 1430, 'bargain': 1431, 'prepare': 1432, 'prices': 1433, 'meter': 1434, 'awkward': 1435, 'complained': 1436, 'precise': 1437, 'harmless': 1438, 'prude': 1439, 'interrupt': 1440, 'mercy': 1441, 'dreamer': 1442, 'lay': 1443, 'learned': 1444, 'tennis': 1445, 'mayor': 1446, 'totally': 1447, 'witness': 1448, 'horrified': 1449, 'embarrassed': 1450, 'handsome': 1451, 'windy': 1452, 'sour': 1453, 'held': 1454, 'repulsive': 1455, 'punched': 1456, 'crack': 1457, 'disloyal': 1458, 'cross': 1459, 'debt': 1460, 'embarrassing': 1461, 'traveling': 1462, 'rarely': 1463, 'aggressive': 1464, 'search': 1465, 'entered': 1466, 'courteous': 1467, 'weren': 1468, 'advance': 1469, 'familiar': 1470, 'pants': 1471, 'weather': 1472, 'barely': 1473, 'unconscious': 1474, 'brainer': 1475, 'completely': 1476, 'wow': 1477, 'bury': 1478, 'waved': 1479, 'aim': 1480, 'tries': 1481, 'jumped': 1482, 'marry': 1483, 'west': 1484, 'aboard': 1485, 'calls': 1486, 'art': 1487, 'biased': 1488, 'fad': 1489, 'replace': 1490, 'flies': 1491, 'sensible': 1492, 'watchful': 1493, 'confessed': 1494, 'beans': 1495, 'rock': 1496, 'recovered': 1497, 'cars': 1498, 'succeeded': 1499, 'jittery': 1500, 'puzzled': 1501, 'sloshed': 1502, 'unarmed': 1503, 'fact': 1504, 'pipe': 1505, 'enter': 1506, 'timing': 1507, 'card': 1508, 'plants': 1509, 'shake': 1510, 'icky': 1511, 'caution': 1512, 'beef': 1513, 'driven': 1514, 'enjoyed': 1515, 'faint': 1516, 'giddy': 1517, 'handled': 1518, 'honey': 1519, 'women': 1520, 'rose': 1521, 'smelled': 1522, 'understood': 1523, 'bleeding': 1524, 'diabetic': 1525, 'drowning': 1526, 'freezing': 1527, 'grounded': 1528, 'rumor': 1529, 'garbage': 1530, 'genuine': 1531, 'snowing': 1532, 'forward': 1533, 'stir': 1534, 'pushing': 1535, 'bores': 1536, 'mature': 1537, 'shock': 1538, 'years': 1539, 'ain': 1540, 'respectful': 1541, 'blow': 1542, 'kite': 1543, 'dropped': 1544, 'sells': 1545, 'caused': 1546, 'contributed': 1547, 'ring': 1548, 'cheese': 1549, 'movies': 1550, 'salmon': 1551, 'hiking': 1552, 'insist': 1553, 'object': 1554, 'respect': 1555, 'plane': 1556, 'contented': 1557, 'expecting': 1558, 'flattered': 1559, 'miserable': 1560, 'observant': 1561, 'saying': 1562, 'coach': 1563, 'helping': 1564, 'takes': 1565, 'test': 1566, 'accurate': 1567, 'personal': 1568, 'gambling': 1569, 'model': 1570, 'differ': 1571, 'cow': 1572, 'oven': 1573, 'texted': 1574, 'growing': 1575, 'racist': 1576, 'guns': 1577, 'mug': 1578, 'video': 1579, 'uncle': 1580, 'keeps': 1581, 'corrected': 1582, 'letter': 1583, 'carrots': 1584, 'secrets': 1585, 'dessert': 1586, 'protest': 1587, 'punished': 1588, 'details': 1589, 'insulted': 1590, 'selected': 1591, 'testify': 1592, 'survivor': 1593, 'making': 1594, 'holiday': 1595, 'optimistic': 1596, 'successful': 1597, 'untalented': 1598, 'miracle': 1599, 'risky': 1600, 'teased': 1601, 'beauty': 1602, 'snap': 1603, 'counting': 1604, 'calmly': 1605, 'sweating': 1606, 'brothers': 1607, 'helpless': 1608, 'capsized': 1609, 'police': 1610, 'trunk': 1611, 'signed': 1612, 'comfortable': 1613, 'dawn': 1614, 'cheating': 1615, 'writes': 1616, 'violence': 1617, 'evidence': 1618, 'homework': 1619, 'supper': 1620, 'hammer': 1621, 'color': 1622, 'minded': 1623, 'modest': 1624, 'happening': 1625, 'necessary': 1626, 'allowed': 1627, 'negotiate': 1628, 'leaves': 1629, 'remove': 1630, 'alarm': 1631, 'texting': 1632, 'church': 1633, 'sky': 1634, 'exit': 1635, 'sipped': 1636, 'wears': 1637, 'ideas': 1638, 'visit': 1639, 'organized': 1640, 'prisoners': 1641, 'stolen': 1642, 'cannot': 1643, 'startled': 1644, 'blushing': 1645, 'barking': 1646, 'drawer': 1647, 'concentrate': 1648, 'juice': 1649, 'blanket': 1650, 'distracted': 1651, 'remained': 1652, 'cap': 1653, 'breakfast': 1654, 'computer': 1655, 'retire': 1656, 'criminal': 1657, 'weight': 1658, 'stressed': 1659, 'belongs': 1660, 'rainy': 1661, 'gain': 1662, 'elaborate': 1663, 'tire': 1664, 'complaining': 1665, 'abandoned': 1666, 'sports': 1667, 'guitar': 1668, 'released': 1669, 'hobby': 1670, 'traitor': 1671, 'obnoxious': 1672, 'screw': 1673, 'question': 1674, 'pet': 1675, 'days': 1676, 'raking': 1677, 'graduate': 1678, 'style': 1679, 'photo': 1680, 'persuaded': 1681, 'wild': 1682, 'downtown': 1683, 'jacket': 1684, 'registered': 1685, 'comic': 1686, 'log': 1687, 'duck': 1688, 'cheers': 1689, 'dozed': 1690, 'snore': 1691, 'skip': 1692, 'east': 1693, 'cringed': 1694, 'exhaled': 1695, 'loyal': 1696, 'sand': 1697, 'seize': 1698, 'swam': 1699, 'carry': 1700, 'fret': 1701, 'south': 1702, 'absurd': 1703, 'tragic': 1704, 'listened': 1705, 'squinted': 1706, 'baking': 1707, 'chubby': 1708, 'hiding': 1709, 'paying': 1710, 'wept': 1711, 'lighten': 1712, 'winced': 1713, 'comfort': 1714, 'guts': 1715, 'mice': 1716, 'hesitated': 1717, 'math': 1718, 'regret': 1719, 'baker': 1720, 'medic': 1721, 'blessed': 1722, 'furious': 1723, 'jail': 1724, 'stuffed': 1725, 'scam': 1726, 'trap': 1727, 'ironic': 1728, 'poison': 1729, 'lift': 1730, 'means': 1731, 'press': 1732, 'release': 1733, 'blushed': 1734, 'grim': 1735, 'pity': 1736, 'attentive': 1737, 'safely': 1738, 'evening': 1739, 'donut': 1740, 'poet': 1741, 'bigot': 1742, 'lousy': 1743, 'opera': 1744, 'sushi': 1745, 'piano': 1746, 'risks': 1747, 'homeless': 1748, 'homesick': 1749, 'angel': 1750, 'rational': 1751, 'truthful': 1752, 'deer': 1753, 'wolf': 1754, 'bedtime': 1755, 'lead': 1756, 'pigs': 1757, 'plug': 1758, 'sued': 1759, 'ink': 1760, 'passed': 1761, 'bossy': 1762, 'included': 1763, 'reasonable': 1764, 'supportive': 1765, 'skate': 1766, 'wins': 1767, 'deeply': 1768, 'braces': 1769, 'pinched': 1770, 'american': 1771, 'admired': 1772, 'muslim': 1773, 'cough': 1774, 'dislike': 1775, 'bananas': 1776, 'freaked': 1777, 'elected': 1778, 'tenure': 1779, 'nature': 1780, 'hockey': 1781, 'rescheduled': 1782, 'ghost': 1783, 'behave': 1784, 'budge': 1785, 'plumber': 1786, 'committed': 1787, 'dedicated': 1788, 'desperate': 1789, 'shivering': 1790, 'skeptical': 1791, 'worse': 1792, 'saturday': 1793, 'relief': 1794, 'business': 1795, 'outdated': 1796, 'seal': 1797, 'sweep': 1798, 'japan': 1799, 'joined': 1800, 'annoyed': 1801, 'page': 1802, 'before': 1803, 'respond': 1804, 'thinks': 1805, 'wicked': 1806, 'qualified': 1807, 'hiring': 1808, 'grass': 1809, 'beard': 1810, 'gambler': 1811, 'honored': 1812, 'nauseous': 1813, 'vision': 1814, 'parties': 1815, 'sisters': 1816, 'flu': 1817, 'college': 1818, 'plead': 1819, 'revenge': 1820, 'careless': 1821, 'guide': 1822, 'beginner': 1823, 'diplomatic': 1824, 'exercising': 1825, 'fascinated': 1826, 'undressing': 1827, 'redundant': 1828, 'larger': 1829, 'biting': 1830, 'appeared': 1831, 'whistling': 1832, 'bunch': 1833, 'hogwash': 1834, 'illegal': 1835, 'rubbish': 1836, 'radio': 1837, 'intervened': 1838, 'unmoved': 1839, 'volunteers': 1840, 'retiring': 1841, 'country': 1842, 'tragedy': 1843, 'causes': 1844, 'wipe': 1845, 'bees': 1846, 'brush': 1847, 'dismissed': 1848, 'bug': 1849, 'overdo': 1850, 'solution': 1851, 'mentioned': 1852, 'studied': 1853, 'husband': 1854, 'photogenic': 1855, 'adventurous': 1856, 'abhor': 1857, 'notice': 1858, 'seizure': 1859, 'resign': 1860, 'leukemia': 1861, 'returned': 1862, 'showered': 1863, 'seldom': 1864, 'solve': 1865, 'able': 1866, 'aware': 1867, 'celebrating': 1868, 'interfering': 1869, 'introverted': 1870, 'vacation': 1871, 'soaking': 1872, 'breathing': 1873, 'unusual': 1874, 'habit': 1875, 'moldy': 1876, 'ago': 1877, 'impressive': 1878, 'irrelevant': 1879, 'terrifying': 1880, 'bottle': 1881, 'orange': 1882, 'assertive': 1883, 'lake': 1884, 'sea': 1885, 'feared': 1886, 'rabbits': 1887, 'immature': 1888, 'panicking': 1889, 'dishes': 1890, 'suffered': 1891, 'pleasure': 1892, 'teaches': 1893, 'naughty': 1894, 'fuse': 1895, 'blown': 1896, 'apply': 1897, 'drinking': 1898, 'teeth': 1899, 'top': 1900, 'street': 1901, 'mention': 1902, 'reason': 1903, 'umbrella': 1904, 'suddenly': 1905, 'arabic': 1906, 'ticket': 1907, 'sneezed': 1908, 'sweat': 1909, 'hybrid': 1910, 'empowered': 1911, 'hay': 1912, 'coughing': 1913, 'beach': 1914, 'egg': 1915, 'month': 1916, 'humiliated': 1917, 'fairly': 1918, 'company': 1919, 'washing': 1920, 'difficult': 1921, 'size': 1922, 'sofa': 1923, 'pure': 1924, 'informed': 1925, 'repair': 1926, 'cards': 1927, 'socks': 1928, 'rejected': 1929, 'wisely': 1930, 'studies': 1931, 'showing': 1932, 'ludicrous': 1933, 'limit': 1934, 'guard': 1935, 'problems': 1936, 'monster': 1937, 'sarcastic': 1938, 'murdered': 1939, 'depend': 1940, 'mistakes': 1941, 'briefly': 1942, 'dozing': 1943, 'heavy': 1944, 'whatever': 1945, 'five': 1946, 'advised': 1947, 'others': 1948, 'thieves': 1949, 'arms': 1950, 'loudly': 1951, 'skipped': 1952, 'bathe': 1953, 'grade': 1954, 'lightly': 1955, 'pulled': 1956, 'discouraged': 1957, 'overwhelmed': 1958, 'garden': 1959, 'heavily': 1960, 'chilly': 1961, 'rage': 1962, 'murder': 1963, 'rising': 1964, 'worst': 1965, 'driver': 1966, 'responsible': 1967, 'european': 1968, 'robots': 1969, 'gloves': 1970, 'offend': 1971, 'address': 1972, 'occur': 1973, 'confirm': 1974, 'offense': 1975, 'knit': 1976, 'nodded': 1977, 'sighed': 1978, 'speed': 1979, 'lies': 1980, 'chill': 1981, 'bark': 1982, 'film': 1983, 'grinned': 1984, 'cured': 1985, 'sober': 1986, 'aah': 1987, 'dig': 1988, 'cpr': 1989, 'relented': 1990, 'pooped': 1991, 'unwell': 1992, 'fox': 1993, 'cd': 1994, 'phony': 1995, 'jesus': 1996, 'mama': 1997, 'scoot': 1998, 'agrees': 1999, 'snores': 2000, 'cds': 2001, 'gloat': 2002, 'shout': 2003, 'coin': 2004, 'adore': 2005, 'exercised': 2006, 'jazz': 2007, 'nailed': 2008, 'taxes': 2009, 'protested': 2010, 'lips': 2011, 'third': 2012, 'widow': 2013, 'falling': 2014, 'fasting': 2015, 'humming': 2016, 'psyched': 2017, 'fatal': 2018, 'rule': 2019, 'urgent': 2020, 'clap': 2021, 'stick': 2022, 'lame': 2023, 'glum': 2024, 'slim': 2025, 'sweated': 2026, 'older': 2027, 'bore': 2028, 'owns': 2029, 'realistic': 2030, 'describe': 2031, 'ghosts': 2032, 'morning': 2033, 'cranky': 2034, 'barbaric': 2035, 'liars': 2036, 'lion': 2037, 'beaten': 2038, 'wonder': 2039, 'member': 2040, 'gullible': 2041, 'reformed': 2042, 'restless': 2043, 'shooting': 2044, 'thinking': 2045, 'ticklish': 2046, 'unbiased': 2047, 'identify': 2048, 'robot': 2049, 'archaic': 2050, 'ladies': 2051, 'staring': 2052, 'applauded': 2053, 'dating': 2054, 'heroes': 2055, 'minors': 2056, 'nerve': 2057, 'scare': 2058, 'slipping': 2059, 'questions': 2060, 'body': 2061, 'security': 2062, 'gamble': 2063, 'circle': 2064, 'exhale': 2065, 'cookie': 2066, 'hole': 2067, 'eats': 2068, 'toys': 2069, 'senior': 2070, 'insecure': 2071, 'pie': 2072, 'belong': 2073, 'steal': 2074, 'queasy': 2075, 'visa': 2076, 'garlic': 2077, 'yellow': 2078, 'violin': 2079, 'funds': 2080, 'rat': 2081, 'bacon': 2082, 'support': 2083, 'mask': 2084, 'dentist': 2085, 'surgeon': 2086, 'addict': 2087, 'easygoing': 2088, 'diet': 2089, 'plastered': 2090, 'sensitive': 2091, 'unmarried': 2092, 'voting': 2093, 'painful': 2094, 'wig': 2095, 'flaw': 2096, 'viral': 2097, 'gorgeous': 2098, 'stealing': 2099, 'offer': 2100, 'cage': 2101, 'gate': 2102, 'beside': 2103, 'shouting': 2104, 'symptoms': 2105, 'vary': 2106, 'dvd': 2107, 'sheep': 2108, 'touchy': 2109, 'sinking': 2110, 'responded': 2111, 'hypocrite': 2112, 'idiots': 2113, 'morons': 2114, 'file': 2115, 'crude': 2116, 'judge': 2117, 'hint': 2118, 'wheel': 2119, 'sold': 2120, 'author': 2121, 'prison': 2122, 'brakes': 2123, 'borrow': 2124, 'spinach': 2125, 'picnics': 2126, 'turtles': 2127, 'answers': 2128, 'data': 2129, 'usually': 2130, 'barefoot': 2131, 'cleaning': 2132, 'detained': 2133, 'outdoors': 2134, 'unharmed': 2135, 'cooperate': 2136, 'bachelor': 2137, 'freshman': 2138, 'musician': 2139, 'dehydrated': 2140, 'illiterate': 2141, 'methodical': 2142, 'quitter': 2143, 'unemployed': 2144, 'bear': 2145, 'likely': 2146, 'snowman': 2147, 'option': 2148, 'beyond': 2149, 'brand': 2150, 'undamaged': 2151, 'ache': 2152, 'peel': 2153, 'choked': 2154, 'alright': 2155, 'evident': 2156, 'logical': 2157, 'flag': 2158, 'fork': 2159, 'ford': 2160, 'callous': 2161, 'immoral': 2162, 'nodding': 2163, 'sobbing': 2164, 'improve': 2165, 'recover': 2166, 'brain': 2167, 'road': 2168, 'plenty': 2169, 'quitting': 2170, 'refugees': 2171, 'adjust': 2172, 'present': 2173, 'whistle': 2174, 'defend': 2175, 'rub': 2176, 'meal': 2177, 'prayed': 2178, 'ambition': 2179, 'painter': 2180, 'gloomy': 2181, 'stared': 2182, 'surfing': 2183, 'lad': 2184, 'southpaw': 2185, 'id': 2186, 'salad': 2187, 'disobeyed': 2188, 'sunburned': 2189, 'football': 2190, 'voice': 2191, 'screams': 2192, 'baseball': 2193, 'teaching': 2194, 'tokyo': 2195, 'patience': 2196, 'misunderstood': 2197, 'helmet': 2198, 'sword': 2199, 'shave': 2200, 'biking': 2201, 'shifted': 2202, 'gears': 2203, 'soundly': 2204, 'taught': 2205, 'refund': 2206, 'acquitted': 2207, 'zoo': 2208, 'escort': 2209, 'rent': 2210, 'tipsy': 2211, 'attorney': 2212, 'daydreaming': 2213, 'defenseless': 2214, 'experienced': 2215, 'extroverted': 2216, 'heartbroken': 2217, 'pickle': 2218, 'resourceful': 2219, 'manager': 2220, 'trustworthy': 2221, 'natural': 2222, 'ages': 2223, 'soft': 2224, 'omen': 2225, 'disgrace': 2226, 'shortcut': 2227, 'impossible': 2228, 'inadequate': 2229, 'toy': 2230, 'sweltering': 2231, 'tax': 2232, 'season': 2233, 'practice': 2234, 'sticky': 2235, 'seeing': 2236, 'changes': 2237, 'lid': 2238, 'calmed': 2239, 'forgave': 2240, 'reads': 2241, 'roll': 2242, 'silence': 2243, 'steady': 2244, 'nosy': 2245, 'bugging': 2246, 'engine': 2247, 'breather': 2248, 'doubtful': 2249, 'peculiar': 2250, 'moon': 2251, 'salt': 2252, 'blocked': 2253, 'greeted': 2254, 'visited': 2255, 'bizarre': 2256, 'times': 2257, 'friday': 2258, 'bagel': 2259, 'dyslexic': 2260, 'resolute': 2261, 'skittish': 2262, 'tactless': 2263, 'wavering': 2264, 'mouse': 2265, 'suffer': 2266, 'toes': 2267, 'teachers': 2268, 'freedom': 2269, 'lack': 2270, 'realize': 2271, 'rented': 2272, 'canadians': 2273, 'impartial': 2274, 'disaster': 2275, 'cost': 2276, 'prize': 2277, 'comb': 2278, 'loo': 2279, 'picky': 2280, 'less': 2281, 'catholic': 2282, 'cane': 2283, 'blinds': 2284, 'consider': 2285, 'wire': 2286, 'dial': 2287, 'deceive': 2288, 'joy': 2289, 'recently': 2290, 'picasso': 2291, 'hunt': 2292, 'grown': 2293, 'bowed': 2294, 'politely': 2295, 'pork': 2296, 'drugs': 2297, 'halloween': 2298, 'surprises': 2299, 'receipt': 2300, 'allergies': 2301, 'standards': 2302, 'toothache': 2303, 'ropes': 2304, 'led': 2305, 'chocolate': 2306, 'elephants': 2307, 'wallet': 2308, 'yolks': 2309, 'view': 2310, 'tires': 2311, 'bar': 2312, 'frustrated': 2313, 'sunday': 2314, 'gladly': 2315, 'between': 2316, 'america': 2317, 'heading': 2318, 'killing': 2319, 'living': 2320, 'shape': 2321, 'riddle': 2322, 'threat': 2323, 'issue': 2324, 'effective': 2325, 'enjoyable': 2326, 'race': 2327, 'sunny': 2328, 'settled': 2329, 'compromise': 2330, 'fleas': 2331, 'smokes': 2332, 'tiny': 2333, 'stomach': 2334, 'parrots': 2335, 'article': 2336, 'tb': 2337, 'disliked': 2338, 'spit': 2339, 'upsetting': 2340, 'storm': 2341, 'tank': 2342, 'ambushed': 2343, 'mystery': 2344, 'excellent': 2345, 'silently': 2346, 'deported': 2347, 'goggles': 2348, 'headed': 2349, 'fingers': 2350, 'considered': 2351, 'visitors': 2352, 'owners': 2353, 'eater': 2354, 'belt': 2355, 'build': 2356, 'hospital': 2357, 'copies': 2358, 'rely': 2359, 'earth': 2360, 'goodnight': 2361, 'brown': 2362, 'gentleman': 2363, 'raised': 2364, 'shook': 2365, 'brushed': 2366, 'intervene': 2367, 'scarf': 2368, 'schedule': 2369, 'privacy': 2370, 'everywhere': 2371, 'colors': 2372, 'handling': 2373, 'guessing': 2374, 'sort': 2375, 'dubious': 2376, 'inflation': 2377, 'depends': 2378, 'cumbersome': 2379, 'greasy': 2380, 'moot': 2381, 'sight': 2382, 'rats': 2383, 'reconsider': 2384, 'pretend': 2385, 'teen': 2386, 'aspirin': 2387, 'error': 2388, 'foxes': 2389, 'articulate': 2390, 'recovering': 2391, 'hopeful': 2392, 'future': 2393, 'cowards': 2394, 'terms': 2395, 'stain': 2396, 'applause': 2397, 'pockets': 2398, 'invite': 2399, 'choosy': 2400, 'information': 2401, 'religion': 2402, 'challenge': 2403, 'candlelight': 2404, 'outfit': 2405, 'spilled': 2406, 'boyfriend': 2407, 'hop': 2408, 'froze': 2409, 'swore': 2410, 'kick': 2411, 'cheer': 2412, 'goofed': 2413, 'moaned': 2414, 'tidy': 2415, 'dj': 2416, 'sexy': 2417, 'humor': 2418, 'blinked': 2419, 'groaned': 2420, 'inhaled': 2421, 'tripped': 2422, 'leo': 2423, 'above': 2424, 'below': 2425, 'higher': 2426, 'bottoms': 2427, 'swiss': 2428, 'human': 2429, 'objected': 2430, 'shrugged': 2431, 'threw': 2432, 'whistled': 2433, 'immune': 2434, 'soaked': 2435, 'wasted': 2436, 'bogus': 2437, 'magic': 2438, 'chat': 2439, 'braked': 2440, 'gasped': 2441, 'unscrew': 2442, 'sunk': 2443, 'specific': 2444, 'bowl': 2445, 'stare': 2446, 'flip': 2447, 'hunk': 2448, 'nerd': 2449, 'slob': 2450, 'monk': 2451, 'buried': 2452, 'oppose': 2453, 'snickered': 2454, 'surrender': 2455, 'frantic': 2456, 'neutral': 2457, 'stumped': 2458, 'foggy': 2459, 'yen': 2460, 'futile': 2461, 'warmer': 2462, 'across': 2463, 'burns': 2464, 'record': 2465, 'settle': 2466, 'neat': 2467, 'wary': 2468, 'drag': 2469, 'loss': 2470, 'ship': 2471, 'ramble': 2472, 'slouch': 2473, 'examine': 2474, 'avoids': 2475, 'heroic': 2476, 'dare': 2477, 'chased': 2478, 'jogging': 2479, 'honor': 2480, 'trips': 2481, 'space': 2482, 'rugby': 2483, 'rewrote': 2484, 'wimped': 2485, 'gemini': 2486, 'taurus': 2487, 'barber': 2488, 'dancer': 2489, 'surfer': 2490, 'adult': 2491, 'autistic': 2492, 'cultured': 2493, 'eighteen': 2494, 'managing': 2495, 'iron': 2496, 'amazed': 2497, 'vague': 2498, 'curse': 2499, 'complex': 2500, 'review': 2501, 'itch': 2502, 'shrieked': 2503, 'pouting': 2504, 'whining': 2505, 'yawning': 2506, 'approve': 2507, 'enlisted': 2508, 'grumbled': 2509, 'spy': 2510, 'obese': 2511, 'vegan': 2512, 'ufo': 2513, 'starve': 2514, 'adults': 2515, 'doomed': 2516, 'vanished': 2517, 'imbecile': 2518, 'moody': 2519, 'futon': 2520, 'blindfold': 2521, 'despair': 2522, 'peas': 2523, 'dies': 2524, 'haircut': 2525, 'ham': 2526, 'cracked': 2527, 'skating': 2528, 'bled': 2529, 'honk': 2530, 'horn': 2531, 'thrilling': 2532, 'runner': 2533, 'borrowed': 2534, 'bribed': 2535, 'designed': 2536, 'reborn': 2537, 'scammed': 2538, 'asthma': 2539, 'orders': 2540, 'rights': 2541, 'camels': 2542, 'cities': 2543, 'grapes': 2544, 'poetry': 2545, 'trains': 2546, 'sock': 2547, 'autumn': 2548, 'crew': 2549, 'lamp': 2550, 'crushed': 2551, 'dazzled': 2552, 'drugged': 2553, 'poems': 2554, 'songs': 2555, 'scorpio': 2556, 'soldier': 2557, 'tourist': 2558, 'widower': 2559, 'adaptable': 2560, 'orphan': 2561, 'bilingual': 2562, 'efficient': 2563, 'strike': 2564, 'surviving': 2565, 'owner': 2566, 'gag': 2567, 'bomb': 2568, 'superb': 2569, 'worries': 2570, 'rental': 2571, 'homemade': 2572, 'midnight': 2573, 'occupied': 2574, 'poisoned': 2575, 'suicidal': 2576, 'hustling': 2577, 'aloud': 2578, 'russia': 2579, 'ease': 2580, 'clapping': 2581, 'spitting': 2582, 'worrying': 2583, 'straighten': 2584, 'myth': 2585, 'doable': 2586, 'untrue': 2587, 'asian': 2588, 'spies': 2589, 'false': 2590, 'crept': 2591, 'online': 2592, 'unhurt': 2593, 'upbeat': 2594, 'winded': 2595, 'rushed': 2596, 'staggered': 2597, 'frank': 2598, 'adopted': 2599, 'choking': 2600, 'elderly': 2601, 'violent': 2602, 'wounded': 2603, 'snoring': 2604, 'wax': 2605, 'prevail': 2606, 'group': 2607, 'sync': 2608, 'lawyers': 2609, 'stalled': 2610, 'bummer': 2611, 'fiasco': 2612, 'pushed': 2613, 'legibly': 2614, 'charlatan': 2615, 'earned': 2616, 'hoot': 2617, 'snob': 2618, 'unkind': 2619, 'avoid': 2620, 'clich': 2621, 'juggle': 2622, 'recycle': 2623, 'hasty': 2624, 'struggle': 2625, 'tease': 2626, 'tempt': 2627, 'dude': 2628, 'afternoon': 2629, 'lip': 2630, 'counts': 2631, 'dislikes': 2632, 'flew': 2633, 'writer': 2634, 'actor': 2635, 'ex': 2636, 'banana': 2637, 'decorated': 2638, 'flunk': 2639, 'gossip': 2640, 'suntan': 2641, 'expelled': 2642, 'stroke': 2643, 'mondays': 2644, 'sundays': 2645, 'hiccups': 2646, 'voices': 2647, 'hiccup': 2648, 'bowling': 2649, 'history': 2650, 'lobster': 2651, 'mahjong': 2652, 'puzzles': 2653, 'stories': 2654, 'predicted': 2655, 'sneeze': 2656, 'korean': 2657, 'puppy': 2658, 'justice': 2659, 'hammered': 2660, 'impolite': 2661, 'agony': 2662, 'pardoned': 2663, 'prosper': 2664, 'against': 2665, 'dependable': 2666, 'firing': 2667, 'meditating': 2668, 'bitter': 2669, 'overweight': 2670, 'prejudiced': 2671, 'winner': 2672, 'forgotten': 2673, 'wednesday': 2674, 'noon': 2675, 'revolting': 2676, 'sickening': 2677, 'searching': 2678, 'lend': 2679, 'jaw': 2680, 'tooth': 2681, 'sir': 2682, 'tune': 2683, 'science': 2684, 'bach': 2685, 'sings': 2686, 'grumbling': 2687, 'quibbling': 2688, 'sniffling': 2689, 'tower': 2690, 'barked': 2691, 'roof': 2692, 'leaks': 2693, 'siren': 2694, 'snag': 2695, 'annoys': 2696, 'bites': 2697, 'theirs': 2698, 'tricky': 2699, 'dice': 2700, 'pear': 2701, 'crook': 2702, 'moron': 2703, 'balding': 2704, 'sitting': 2705, 'smarter': 2706, 'unfazed': 2707, 'retaliated': 2708, 'clueless': 2709, 'crash': 2710, 'blast': 2711, 'guests': 2712, 'landed': 2713, 'action': 2714, 'partners': 2715, 'stronger': 2716, 'rip': 2717, 'taken': 2718, 'paris': 2719, 'wind': 2720, 'disgust': 2721, 'screwed': 2722, 'amusing': 2723, 'elusive': 2724, 'obscene': 2725, 'pro': 2726, 'insured': 2727, 'hibernate': 2728, 'bullet': 2729, 'boil': 2730, 'boxes': 2731, 'nails': 2732, 'hatch': 2733, 'jam': 2734, 'stingy': 2735, 'interfere': 2736, 'remind': 2737, 'fetch': 2738, 'popcorn': 2739, 'bottom': 2740, 'acts': 2741, 'ten': 2742, 'slowpoke': 2743, 'humiliating': 2744, 'interesting': 2745, 'compete': 2746, 'chickened': 2747, 'downloaded': 2748, 'feverish': 2749, 'isolated': 2750, 'tomatoes': 2751, 'fanatics': 2752, 'heat': 2753, 'weddings': 2754, 'bruise': 2755, 'laptop': 2756, 'epilepsy': 2757, 'army': 2758, 'outwitted': 2759, 'land': 2760, 'scored': 2761, 'vitamins': 2762, 'dollar': 2763, 'mommy': 2764, 'assaulted': 2765, 'convicted': 2766, 'wondering': 2767, 'arrange': 2768, 'assist': 2769, 'double': 2770, 'reward': 2771, 'detective': 2772, 'foreigner': 2773, 'begging': 2774, 'blaming': 2775, 'catching': 2776, 'cooperating': 2777, 'ending': 2778, 'hardworking': 2779, 'training': 2780, 'handed': 2781, 'witch': 2782, 'ammo': 2783, 'persevering': 2784, 'spontaneous': 2785, 'sympathetic': 2786, 'unambitious': 2787, 'unconvinced': 2788, 'river': 2789, 'seven': 2790, 'tasted': 2791, 'handmade': 2792, 'acceptable': 2793, 'outrage': 2794, 'inevitable': 2795, 'misleading': 2796, 'typical': 2797, 'sufficient': 2798, 'humid': 2799, 'paging': 2800, 'covered': 2801, 'lemons': 2802, 'improvise': 2803, 'niece': 2804, 'merry': 2805, 'jeans': 2806, 'shift': 2807, 'squeak': 2808, 'neither': 2809, 'pour': 2810, 'rejection': 2811, 'graceful': 2812, 'load': 2813, 'chances': 2814, 'frankly': 2815, 'puzzle': 2816, 'net': 2817, 'tub': 2818, 'cure': 2819, 'pilots': 2820, 'gifts': 2821, 'wasting': 2822, 'acted': 2823, 'client': 2824, 'psycho': 2825, 'tailor': 2826, 'dejected': 2827, 'deranged': 2828, 'giggling': 2829, 'knocking': 2830, 'dummy': 2831, 'perverse': 2832, 'pleasant': 2833, 'spirited': 2834, 'unafraid': 2835, 'unnerved': 2836, 'stops': 2837, 'sees': 2838, 'sleeps': 2839, 'slowed': 2840, 'front': 2841, 'fools': 2842, 'giving': 2843, 'neighbors': 2844, 'newcomers': 2845, 'newlyweds': 2846, 'resigning': 2847, 'survivors': 2848, 'unrelated': 2849, 'treasure': 2850, 'oar': 2851, 'devil': 2852, 'menace': 2853, 'than': 2854, 'wings': 2855, 'accidents': 2856, 'golfer': 2857, 'maniac': 2858, 'typist': 2859, 'wizard': 2860, 'boxing': 2861, 'brace': 2862, 'normally': 2863, 'spell': 2864, 'immediately': 2865, 'supply': 2866, 'delicately': 2867, 'disturb': 2868, 'yell': 2869, 'meals': 2870, 'veggies': 2871, 'hamster': 2872, 'costs': 2873, 'gardening': 2874, 'without': 2875, 'stairs': 2876, 'flow': 2877, 'croissant': 2878, 'backward': 2879, 'pajamas': 2880, 'hunting': 2881, 'oranges': 2882, 'moves': 2883, 'ratted': 2884, 'bartender': 2885, 'biologist': 2886, 'senile': 2887, 'intelligent': 2888, 'struck': 2889, 'heaven': 2890, 'professor': 2891, 'dumbfounded': 2892, 'brazil': 2893, 'awoke': 2894, 'reschedule': 2895, 'reach': 2896, 'confiscated': 2897, 'protected': 2898, 'ripped': 2899, 'chemistry': 2900, 'computers': 2901, 'hypocrisy': 2902, 'paperwork': 2903, 'website': 2904, 'gunshots': 2905, 'novel': 2906, 'ocean': 2907, 'temper': 2908, 'adventure': 2909, 'hedgehogs': 2910, 'soul': 2911, 'band': 2912, 'kleenex': 2913, 'recommend': 2914, 'rematch': 2915, 'vengeance': 2916, 'stranger': 2917, 'unprepared': 2918, 'demonstrate': 2919, 'clerk': 2920, 'conservative': 2921, 'disorganized': 2922, 'dissatisfied': 2923, 'homeschooled': 2924, 'ignoring': 2925, 'super': 2926, 'volunteering': 2927, 'valuable': 2928, 'trick': 2929, 'freaks': 2930, 'virus': 2931, 'hype': 2932, 'mandatory': 2933, 'dodge': 2934, 'complicated': 2935, 'drug': 2936, 'camping': 2937, 'couch': 2938, 'lower': 2939, 'planet': 2940, 'pulse': 2941, 'windows': 2942, 'button': 2943, 'bellyaching': 2944, 'hassling': 2945, 'defeated': 2946, 'despises': 2947, 'obstinate': 2948, 'rode': 2949, 'camel': 2950, 'worships': 2951, 'shots': 2952, 'skunks': 2953, 'nit': 2954, 'picking': 2955, 'million': 2956, 'brilliant': 2957, 'curtain': 2958, 'rough': 2959, 'switch': 2960, 'icy': 2961, 'scoffed': 2962, 'artists': 2963, 'required': 2964, 'bounced': 2965, 'detests': 2966, 'forgives': 2967, 'glanced': 2968, 'chickens': 2969, 'delirious': 2970, 'indignant': 2971, 'merciless': 2972, 'observing': 2973, 'sighing': 2974, 'tulips': 2975, 'overheard': 2976, 'stepped': 2977, 'tickled': 2978, 'watches': 2979, 'innovative': 2980, 'instead': 2981, 'slower': 2982, 'competed': 2983, 'demand': 2984, 'warning': 2985, 'pictures': 2986, 'soldiers': 2987, 'classmates': 2988, 'nightmare': 2989, 'seek': 2990, 'score': 2991, 'mirror': 2992, 'wherever': 2993, 'cheaper': 2994, 'developed': 2995, 'lady': 2996, 'notified': 2997, 'dyes': 2998, 'nicely': 2999, 'x': 3000, 'marks': 3001, 'younger': 3002, 'seventh': 3003, 'unethical': 3004, 'zip': 3005, 'admission': 3006, 'steps': 3007, 'bachelors': 3008, 'religious': 3009, 'policeman': 3010, 'ambulance': 3011, 'advise': 3012, 'link': 3013, 'cocaine': 3014, 'startle': 3015, 'disable': 3016, 'ants': 3017, 'kratom': 3018, 'threaten': 3019, 'haste': 3020, 'gives': 3021, 'grabbed': 3022, 'blond': 3023, 'physicist': 3024, 'scientist': 3025, 'forty': 3026, 'frugally': 3027, 'requested': 3028, 'sailor': 3029, 'russian': 3030, 'travels': 3031, 'twisted': 3032, 'imprisoned': 3033, 'journalist': 3034, 'pink': 3035, 'skin': 3036, 'handrail': 3037, 'disappointing': 3038, 'spelled': 3039, 'bolted': 3040, 'verify': 3041, 'clothes': 3042, 'bags': 3043, 'tutor': 3044, 'accident': 3045, 'other': 3046, 'mosquitoes': 3047, 'passport': 3048, 'proposal': 3049, 'chest': 3050, 'regrets': 3051, 'footsteps': 3052, 'intend': 3053, 'hats': 3054, 'lived': 3055, 'outlive': 3056, 'ignorance': 3057, 'muscle': 3058, 'jest': 3059, 'inchworm': 3060, 'victory': 3061, 'shouldn': 3062, 'sometimes': 3063, 'notebook': 3064, 'apology': 3065, 'passenger': 3066, 'discredited': 3067, 'total': 3068, 'wreck': 3069, 'awfully': 3070, 'concentrating': 3071, 'flabbergasted': 3072, 'lab': 3073, 'neighbor': 3074, 'insure': 3075, 'nonstop': 3076, 'horrendous': 3077, 'destiny': 3078, 'flimsy': 3079, 'conspiracy': 3080, 'world': 3081, 'emergency': 3082, 'knock': 3083, 'investigate': 3084, 'prediction': 3085, 'mosquitos': 3086, 'shoulder': 3087, 'shoulders': 3088, 'misses': 3089, 'none': 3090, 'peaches': 3091, 'unsociable': 3092, 'bitterly': 3093, 'flower': 3094, 'badgering': 3095, 'exaggerating': 3096, 'following': 3097, 'harassing': 3098, 'interrupting': 3099, 'wood': 3100, 'rings': 3101, 'wall': 3102, 'crowd': 3103, 'bland': 3104, 'lights': 3105, 'lovers': 3106, 'thick': 3107, 'hunted': 3108, 'tortured': 3109, 'identical': 3110, 'appalling': 3111, 'offensive': 3112, 'permanent': 3113, 'fishy': 3114, 'praying': 3115, 'claims': 3116, 'deserves': 3117, 'freed': 3118, 'imitated': 3119, 'believer': 3120, 'despondent': 3121, 'protective': 3122, 'remarkable': 3123, 'crawling': 3124, 'knocked': 3125, 'peeked': 3126, 'peered': 3127, 'recognized': 3128, 'patiently': 3129, 'sensed': 3130, 'strangled': 3131, 'suffocating': 3132, 'hose': 3133, 'beers': 3134, 'flaws': 3135, 'p': 3136, 'due': 3137, 'killers': 3138, 'revelation': 3139, 'happiness': 3140, 'major': 3141, 'stitches': 3142, 'prisoner': 3143, 'hurting': 3144, 'unpleasant': 3145, 'mocking': 3146, 'fascinate': 3147, 'flute': 3148, 'subject': 3149, 'tells': 3150, 'blog': 3151, 'kabuki': 3152, 'stones': 3153, 'king': 3154, 'weekend': 3155, 'blanks': 3156, 'desire': 3157, 'traffic': 3158, 'breathed': 3159, 'cartwheel': 3160, 'gripped': 3161, 'alzheimer': 3162, 'colleague': 3163, 'memory': 3164, 'unimpressed': 3165, 'aristocrat': 3166, 'print': 3167, 'position': 3168, 'sin': 3169, 'earn': 3170, 'forced': 3171, 'loads': 3172, 'goat': 3173, 'roommate': 3174, 'carpet': 3175, 'spare': 3176, 'tickets': 3177, 'explosions': 3178, 'emailed': 3179, 'custom': 3180, 'mission': 3181, 'mathematics': 3182, 'raspberries': 3183, 'pot': 3184, 'roast': 3185, 'blamed': 3186, 'oiled': 3187, 'report': 3188, 'repeated': 3189, 'kissing': 3190, 'sliced': 3191, 'suggest': 3192, 'fled': 3193, 'pair': 3194, 'buzz': 3195, 'cuff': 3196, 'cursed': 3197, 'frowned': 3198, 'gloated': 3199, 'grunted': 3200, 'prepaid': 3201, 'shouted': 3202, 'needy': 3203, 'poured': 3204, 'dived': 3205, 'knits': 3206, 'limps': 3207, 'rocks': 3208, 'r': 3209, 'b': 3210, 'shivered': 3211, 'flabby': 3212, 'begun': 3213, 'bulky': 3214, 'taboo': 3215, 'lasts': 3216, 'cheats': 3217, 'cusses': 3218, 'farted': 3219, 'sobbed': 3220, 'yawned': 3221, 'blink': 3222, 'nasty': 3223, 'faking': 3224, 'taller': 3225, 'assume': 3226, 'tapes': 3227, 'glue': 3228, 'coup': 3229, 'curt': 3230, 'gambles': 3231, 'gargled': 3232, 'meek': 3233, 'vain': 3234, 'listens': 3235, 'reacted': 3236, 'sneered': 3237, 'sniffed': 3238, 'snorted': 3239, 'hell': 3240, 'dump': 3241, 'heel': 3242, 'drew': 3243, 'abandon': 3244, 'backup': 3245, 'litter': 3246, 'warmly': 3247, 'exist': 3248, 'spoon': 3249, 'mocked': 3250, 'caviar': 3251, 'carded': 3252, 'ax': 3253, 'hives': 3254, 'improvised': 3255, 'suppose': 3256, 'sympathize': 3257, 'canned': 3258, 'framed': 3259, 'weighed': 3260, 'priest': 3261, 'purist': 3262, 'agent': 3263, 'famished': 3264, 'hungover': 3265, 'rebel': 3266, 'worn': 3267, 'solid': 3268, 'elk': 3269, 'noisy': 3270, 'heal': 3271, 'cinch': 3272, 'plant': 3273, 'setup': 3274, 'hearsay': 3275, 'suicide': 3276, 'bounce': 3277, 'mull': 3278, 'hip': 3279, 'cared': 3280, 'louder': 3281, 'filming': 3282, 'gawking': 3283, 'whiff': 3284, 'trash': 3285, 'rare': 3286, 'spam': 3287, 'approves': 3288, 'insisted': 3289, 'nut': 3290, 'pudgy': 3291, 'stoic': 3292, 'sniffled': 3293, 'stumbled': 3294, 'stutters': 3295, 'arabs': 3296, 'amuse': 3297, 'gross': 3298, 'drown': 3299, 'mock': 3300, 'sass': 3301, 'healthily': 3302, 'bloom': 3303, 'riddance': 3304, 'advises': 3305, 'dug': 3306, 'british': 3307, 'grouch': 3308, 'jesuit': 3309, 'tycoon': 3310, 'delicious': 3311, 'acquired': 3312, 'added': 3313, 'classes': 3314, 'dye': 3315, 'poker': 3316, 'exaggerated': 3317, 'drowsy': 3318, 'forbid': 3319, 'bonus': 3320, 'fleeced': 3321, 'crowds': 3322, 'flying': 3323, 'sirens': 3324, 'clocks': 3325, 'prunes': 3326, 'crepes': 3327, 'yacht': 3328, 'squash': 3329, 'resent': 3330, 'queen': 3331, 'firefox': 3332, 'pony': 3333, 'nights': 3334, 'trainee': 3335, 'parole': 3336, 'resentful': 3337, 'damaged': 3338, 'leather': 3339, 'bat': 3340, 'sickens': 3341, 'hoax': 3342, 'parody': 3343, 'sequel': 3344, 'indecent': 3345, 'instinct': 3346, 'obsolete': 3347, 'unlikely': 3348, 'paddling': 3349, 'mortal': 3350, 'motion': 3351, 'aches': 3352, 'lungs': 3353, 'active': 3354, 'obeys': 3355, 'livid': 3356, 'cutie': 3357, 'frowning': 3358, 'scowling': 3359, 'heap': 3360, 'saturn': 3361, 'rad': 3362, 'unreal': 3363, 'mooed': 3364, 'embraced': 3365, 'pun': 3366, 'basic': 3367, 'exercises': 3368, 'graduated': 3369, 'goats': 3370, 'hick': 3371, 'sexist': 3372, 'somber': 3373, 'stable': 3374, 'overdosed': 3375, 'shuddered': 3376, 'stiffened': 3377, 'tensed': 3378, 'testified': 3379, 'untie': 3380, 'assumed': 3381, 'trees': 3382, 'tools': 3383, 'opposed': 3384, 'candy': 3385, 'rebuild': 3386, 'anxious': 3387, 'buddies': 3388, 'winners': 3389, 'matured': 3390, 'hassle': 3391, 'disagrees': 3392, 'absent': 3393, 'coke': 3394, 'suspect': 3395, 'intruding': 3396, 'boats': 3397, 'sink': 3398, 'shovel': 3399, 'contact': 3400, 'tigers': 3401, 'purr': 3402, 'force': 3403, 'farts': 3404, 'gasps': 3405, 'lines': 3406, 'birthday': 3407, 'holidays': 3408, 'tan': 3409, 'gazed': 3410, 'bankrupt': 3411, 'jelly': 3412, 'owes': 3413, 'respects': 3414, 'hungarian': 3415, 'con': 3416, 'outlaw': 3417, 'henpecked': 3418, 'ached': 3419, 'perceptive': 3420, 'nail': 3421, 'draft': 3422, 'seasick': 3423, 'snubbed': 3424, 'guarantee': 3425, 'mishap': 3426, 'hailed': 3427, 'insects': 3428, 'ironing': 3429, 'karaoke': 3430, 'zealots': 3431, 'amnesia': 3432, 'thud': 3433, 'almonds': 3434, 'castles': 3435, 'oysters': 3436, 'sashimi': 3437, 'seafood': 3438, 'upstate': 3439, 'lasagna': 3440, 'puppies': 3441, 'sunsets': 3442, 'muffins': 3443, 'decline': 3444, 'stamp': 3445, 'insulin': 3446, 'asap': 3447, 'surgery': 3448, 'cattle': 3449, 'serve': 3450, 'urge': 3451, 'memo': 3452, 'scold': 3453, 'merchant': 3454, 'minister': 3455, 'salesman': 3456, 'thumbs': 3457, 'farsighted': 3458, 'remodeling': 3459, 'killer': 3460, 'credible': 3461, 'common': 3462, 'disgusts': 3463, 'fluke': 3464, 'magical': 3465, 'planned': 3466, 'classic': 3467, 'firefly': 3468, 'ambush': 3469, 'dishonest': 3470, 'drizzling': 3471, 'forbidden': 3472, 'grotesque': 3473, 'cream': 3474, 'inspiring': 3475, 'vibrating': 3476, 'worthless': 3477, 'clues': 3478, 'mum': 3479, 'joints': 3480, 'itches': 3481, 'tummy': 3482, 'ow': 3483, 'ditched': 3484, 'brains': 3485, 'nurse': 3486, 'prudish': 3487, 'swims': 3488, 'types': 3489, 'hottie': 3490, 'looker': 3491, 'gossiping': 3492, 'poking': 3493, 'resisting': 3494, 'plastic': 3495, 'honked': 3496, 'melted': 3497, 'muddy': 3498, 'quarreled': 3499, 'babies': 3500, 'unsafe': 3501, 'hectic': 3502, 'draws': 3503, 'issues': 3504, 'scurvy': 3505, 'talent': 3506, 'dwarf': 3507, 'felon': 3508, 'fraud': 3509, 'giant': 3510, 'lefty': 3511, 'mason': 3512, 'pilot': 3513, 'shark': 3514, 'cynical': 3515, 'devoted': 3516, 'pompous': 3517, 'radical': 3518, 'shaving': 3519, 'slender': 3520, 'spoiled': 3521, 'unkempt': 3522, 'tofu': 3523, 'maps': 3524, 'fuming': 3525, 'stranded': 3526, 'troubled': 3527, 'unstable': 3528, 'sauce': 3529, 'rear': 3530, 'duet': 3531, 'couple': 3532, 'butchers': 3533, 'buffoon': 3534, 'letdown': 3535, 'atm': 3536, 'reinstated': 3537, 'suggestions': 3538, 'purple': 3539, 'balls': 3540, 'round': 3541, 'bears': 3542, 'anytime': 3543, 'clip': 3544, 'towards': 3545, 'crows': 3546, 'rap': 3547, 'freak': 3548, 'insult': 3549, 'danced': 3550, 'fasten': 3551, 'whirl': 3552, 'onto': 3553, 'daddy': 3554, 'toyota': 3555, 'racket': 3556, 'intrigues': 3557, 'ethiopian': 3558, 'sweets': 3559, 'puts': 3560, 'airs': 3561, 'whisky': 3562, 'shouts': 3563, 'swindled': 3564, 'comedian': 3565, 'frat': 3566, 'gardener': 3567, 'newcomer': 3568, 'helium': 3569, 'mud': 3570, 'hoist': 3571, 'sails': 3572, 'unfortunate': 3573, 'hypnotism': 3574, 'egypt': 3575, 'booked': 3576, 'carp': 3577, 'dealt': 3578, 'dried': 3579, 'funerals': 3580, 'politics': 3581, 'raccoons': 3582, 'reptiles': 3583, 'rug': 3584, 'tattoo': 3585, 'alibi': 3586, 'dandruff': 3587, 'diabetes': 3588, 'immunity': 3589, 'thump': 3590, 'rains': 3591, 'elbow': 3592, 'imagined': 3593, 'area': 3594, 'lent': 3595, 'cartoons': 3596, 'westerns': 3597, 'interest': 3598, 'barbecue': 3599, 'comedies': 3600, 'eggplant': 3601, 'pancakes': 3602, 'brownies': 3603, 'misjudged': 3604, 'internet': 3605, 'caffeine': 3606, 'guidance': 3607, 'tape': 3608, 'bills': 3609, 'orchids': 3610, 'relied': 3611, 'respected': 3612, 'giraffe': 3613, 'pattern': 3614, 'parrot': 3615, 'coma': 3616, 'patrol': 3617, 'petrified': 3618, 'capricorn': 3619, 'cameraman': 3620, 'carpenter': 3621, 'chauffeur': 3622, 'geologist': 3623, 'housewife': 3624, 'lifeguard': 3625, 'masochist': 3626, 'owl': 3627, 'stutterer': 3628, 'engineer': 3629, 'cracking': 3630, 'frying': 3631, 'nearsighted': 3632, 'deck': 3633, 'replaceable': 3634, 'captain': 3635, 'hazardous': 3636, 'poisonous': 3637, 'zebra': 3638, 'ethical': 3639, 'poorly': 3640, 'burnt': 3641, 'enticing': 3642, 'mushroom': 3643, 'travesty': 3644, 'artificial': 3645, 'dirt': 3646, 'improbable': 3647, 'loan': 3648, 'secure': 3649, 'refreshing': 3650, 'narrow': 3651, 'scarce': 3652, 'updated': 3653, 'knead': 3654, 'dough': 3655, 'koalas': 3656, 'celebrate': 3657, 'kites': 3658, 'shrank': 3659, 'natto': 3660, 'october': 3661, 'wonders': 3662, 'planning': 3663, 'rake': 3664, 'saddle': 3665, 'sharks': 3666, 'brazilian': 3667, 'silk': 3668, 'spanish': 3669, 'nagging': 3670, 'squabbling': 3671, 'whimpering': 3672, 'pagoda': 3673, 'shadow': 3674, 'damp': 3675, 'cast': 3676, 'growled': 3677, 'nigh': 3678, 'stank': 3679, 'howled': 3680, 'actors': 3681, 'melons': 3682, 'interns': 3683, 'farce': 3684, 'lance': 3685, 'private': 3686, 'reality': 3687, 'pleases': 3688, 'each': 3689, 'burps': 3690, 'dances': 3691, 'exaggerates': 3692, 'ipad': 3693, 'jockey': 3694, 'madman': 3695, 'player': 3696, 'prince': 3697, 'welder': 3698, 'adapting': 3699, 'agitated': 3700, 'anorexic': 3701, 'athletic': 3702, 'cautious': 3703, 'childish': 3704, 'depraved': 3705, 'drenched': 3706, 'eloquent': 3707, 'immobile': 3708, 'insolent': 3709, 'likeable': 3710, 'reckless': 3711, 'sneezing': 3712, 'studious': 3713, 'tireless': 3714, 'tolerant': 3715, 'learns': 3716, 'dazed': 3717, 'falls': 3718, 'moose': 3719, 'photos': 3720, 'understands': 3721, 'evasive': 3722, 'moaning': 3723, 'paralyzed': 3724, 'yawn': 3725, 'walls': 3726, 'declared': 3727, 'raw': 3728, 'ponchos': 3729, 'waffles': 3730, 'retreat': 3731, 'experts': 3732, 'shared': 3733, 'farmers': 3734, 'sailing': 3735, 'camp': 3736, 'attorneys': 3737, 'comedians': 3738, 'fixing': 3739, 'gardeners': 3740, 'gentlemen': 3741, 'incorrect': 3742, 'past': 3743, 'relatives': 3744, 'soulmates': 3745, 'blessing': 3746, 'prospect': 3747, 'weakling': 3748, 'stake': 3749, 'gpa': 3750, 'jello': 3751, 'unsure': 3752, 'mustn': 3753, 'rut': 3754, 'invisible': 3755, 'chef': 3756, 'atlantis': 3757, 'snakes': 3758, 'beaks': 3759, 'climb': 3760, 'consult': 3761, 'bridge': 3762, 'pasta': 3763, 'rashly': 3764, 'exaggerate': 3765, 'gawk': 3766, 'faces': 3767, 'mislead': 3768, 'sometime': 3769, 'groans': 3770, 'gossips': 3771, 'floating': 3772, 'purpose': 3773, 'useful': 3774, 'garage': 3775, 'hobo': 3776, 'speech': 3777, 'joker': 3778, 'acrobat': 3779, 'shares': 3780, 'france': 3781, 'tenacious': 3782, 'winked': 3783, 'lover': 3784, 'historian': 3785, 'nonsmoker': 3786, 'sophomore': 3787, 'hone': 3788, 'skills': 3789, 'melodramatic': 3790, 'norway': 3791, 'weekly': 3792, 'appreciated': 3793, 'bumped': 3794, 'trout': 3795, 'deliver': 3796, 'pizzas': 3797, 'disregarded': 3798, 'dilemma': 3799, 'refreshed': 3800, 'filled': 3801, 'drift': 3802, 'goosebumps': 3803, 'texts': 3804, 'reasons': 3805, 'fireworks': 3806, 'diploma': 3807, 'grenade': 3808, 'request': 3809, 'toaster': 3810, 'heartburn': 3811, 'insurance': 3812, 'pneumonia': 3813, 'wavy': 3814, 'journal': 3815, 'drill': 3816, 'lifted': 3817, 'astrology': 3818, 'mushrooms': 3819, 'uncles': 3820, 'mysteries': 3821, 'pop': 3822, 'europe': 3823, 'papers': 3824, 'australia': 3825, 'astronomy': 3826, 'libraries': 3827, 'fortune': 3828, 'spaghetti': 3829, 'envelopes': 3830, 'soap': 3831, 'subtitles': 3832, 'vent': 3833, 'planted': 3834, 'pointed': 3835, 'label': 3836, 'require': 3837, 'rubbed': 3838, 'umbrellas': 3839, 'lease': 3840, 'strongly': 3841, 'divorce': 3842, 'martini': 3843, 'pal': 3844, 'raccoon': 3845, 'scooter': 3846, 'dumbstruck': 3847, 'victorious': 3848, 'raffle': 3849, 'farm': 3850, 'bookkeeper': 3851, 'riser': 3852, 'programmer': 3853, 'shutterbug': 3854, 'ambidextrous': 3855, 'alcoholic': 3856, 'disappointed': 3857, 'enthusiastic': 3858, 'freaking': 3859, 'fbi': 3860, 'housesitting': 3861, 'attic': 3862, 'inviting': 3863, 'beggar': 3864, 'paranoid': 3865, 'row': 3866, 'youngest': 3867, 'drawn': 3868, 'conscious': 3869, 'accessible': 3870, 'euros': 3871, 'perks': 3872, 'excessive': 3873, 'april': 3874, 'snowstorm': 3875, 'sunflower': 3876, 'tradition': 3877, 'balmy': 3878, 'conceivable': 3879, 'cooler': 3880, 'fetched': 3881, 'frightening': 3882, 'regrettable': 3883, 'transparent': 3884, 'unnecessary': 3885, 'asia': 3886, 'darts': 3887, 'mars': 3888, 'inbox': 3889, 'itchy': 3890, 'sunburn': 3891, 'beats': 3892, 'stung': 3893, 'oil': 3894, 'pets': 3895, 'pump': 3896, 'rome': 3897, 'italy': 3898, 'idolized': 3899, 'scene': 3900, 'repulses': 3901, 'promoted': 3902, 'bench': 3903, 'pitied': 3904, 'apologizing': 3905, 'pinching': 3906, 'taunting': 3907, 'touching': 3908, 'precautions': 3909, 'kindly': 3910, 'explains': 3911, 'avoidable': 3912, 'blackmail': 3913, 'idle': 3914, 'affair': 3915, 'troubling': 3916, 'ticked': 3917, 'creaked': 3918, 'bucked': 3919, 'jar': 3920, 'jury': 3921, 'mass': 3922, 'noes': 3923, 'abated': 3924, 'limits': 3925, 'singers': 3926, 'amateurs': 3927, 'traitors': 3928, 'messy': 3929, 'insanity': 3930, 'official': 3931, 'puzzling': 3932, 'relaxing': 3933, 'sabotage': 3934, 'shameful': 3935, 'shocking': 3936, 'norm': 3937, 'precious': 3938, 'admires': 3939, 'avoided': 3940, 'raft': 3941, 'coughs': 3942, 'fried': 3943, 'rabbit': 3944, 'charisma': 3945, 'freckles': 3946, 'onions': 3947, 'drummer': 3948, 'gourmet': 3949, 'gymnast': 3950, 'hipster': 3951, 'patriot': 3952, 'pianist': 3953, 'realtor': 3954, 'trucker': 3955, 'intern': 3956, 'competent': 3957, 'deceitful': 3958, 'emotional': 3959, 'fanatical': 3960, 'improving': 3961, 'inventive': 3962, 'legendary': 3963, 'nephew': 3964, 'oblivious': 3965, 'campus': 3966, 'overjoyed': 3967, 'perplexed': 3968, 'regretful': 3969, 'reluctant': 3970, 'shameless': 3971, 'suspended': 3972, 'nosey': 3973, 'trembling': 3974, 'ponies': 3975, 'smiles': 3976, 'steals': 3977, 'suggested': 3978, 'swears': 3979, 'abducted': 3980, 'appalled': 3981, 'babbling': 3982, 'cremated': 3983, 'ecstatic': 3984, 'executed': 3985, 'unshaven': 3986, 'berserk': 3987, 'bonkers': 3988, 'headstrong': 3989, 'undefeated': 3990, 'robbery': 3991, 'swordfish': 3992, 'crisis': 3993, 'theory': 3994, 'injuries': 3995, 'evacuate': 3996, 'withdraw': 3997, 'supplies': 3998, 'downsizing': 3999, 'historians': 4000, 'lifeguards': 4001, 'mates': 4002, 'fails': 4003, 'property': 4004, 'razor': 4005, 'uses': 4006, 'hamlet': 4007, 'banned': 4008, 'wool': 4009, 'huh': 4010, 'quote': 4011, 'distant': 4012, 'failure': 4013, 'lunatic': 4014, 'practical': 4015, 'leaf': 4016, 'under': 4017, 'arrest': 4018, 'nests': 4019, 'champagne': 4020, 'chuck': 4021, 'click': 4022, 'compare': 4023, 'imminent': 4024, 'lager': 4025, 'latin': 4026, 'pears': 4027, 'wanna': 4028, 'ignorant': 4029, 'crass': 4030, 'petty': 4031, 'pushy': 4032, 'fuss': 4033, 'fry': 4034, 'rung': 4035, 'blackmailed': 4036, 'carried': 4037, 'chewed': 4038, 'gum': 4039, 'deals': 4040, 'grain': 4041, 'roughly': 4042, 'vase': 4043, 'earns': 4044, 'fills': 4045, 'hanged': 4046, 'pens': 4047, 'ego': 4048, 'daredevil': 4049, 'incompetent': 4050, 'influential': 4051, 'unrealistic': 4052, 'osaka': 4053, 'miles': 4054, 'wakes': 4055, 'rapidly': 4056, 'raging': 4057, 'ballistic': 4058, 'scripts': 4059, 'buff': 4060, 'timer': 4061, 'undergrad': 4062, 'georgia': 4063, 'trusting': 4064, 'heed': 4065, 'warnings': 4066, 'darkened': 4067, 'hamburger': 4068, 'sandwich': 4069, 'baited': 4070, 'hook': 4071, 'bid': 4072, 'cactus': 4073, 'webcam': 4074, 'shelter': 4075, 'upfront': 4076, 'chopin': 4077, 'deleted': 4078, 'app': 4079, 'register': 4080, 'vulnerable': 4081, 'grades': 4082, 'bee': 4083, 'sting': 4084, 'ground': 4085, 'breakdown': 4086, 'hypocrites': 4087, 'interviews': 4088, 'desert': 4089, 'backache': 4090, 'hangover': 4091, 'migraine': 4092, 'sailboat': 4093, 'earache': 4094, 'experience': 4095, 'discs': 4096, 'savings': 4097, 'sleeve': 4098, 'jackpot': 4099, 'navy': 4100, 'bitten': 4101, 'practicing': 4102, 'confidence': 4103, 'adventures': 4104, 'challenges': 4105, 'folk': 4106, 'sauerkraut': 4107, 'spicy': 4108, 'skirt': 4109, 'odds': 4110, 'watermelon': 4111, 'hotel': 4112, 'dam': 4113, 'earring': 4114, 'balance': 4115, 'hamburgers': 4116, 'performing': 4117, 'album': 4118, 'minutes': 4119, 'keyboard': 4120, 'protection': 4121, 'toothpaste': 4122, 'oversleep': 4123, 'abroad': 4124, 'hood': 4125, 'paused': 4126, 'picked': 4127, 'tambourine': 4128, 'volleyball': 4129, 'pruned': 4130, 'recommended': 4131, 'airplane': 4132, 'sunrise': 4133, 'known': 4134, 'swallowed': 4135, 'swept': 4136, 'geography': 4137, 'otherwise': 4138, 'highway': 4139, 'plate': 4140, 'plates': 4141, 'confess': 4142, 'workout': 4143, 'sympathy': 4144, 'mugged': 4145, 'misinformed': 4146, 'outnumbered': 4147, 'trespassing': 4148, 'flirting': 4149, 'harvard': 4150, 'gym': 4151, 'shotgun': 4152, 'sagittarius': 4153, 'trainer': 4154, 'worker': 4155, 'salesperson': 4156, 'accountant': 4157, 'astronomer': 4158, 'boiling': 4159, 'bringing': 4160, 'extremely': 4161, 'irreplaceable': 4162, 'materialistic': 4163, 'strongest': 4164, 'uncomfortable': 4165, 'evicted': 4166, 'diarrhea': 4167, 'pride': 4168, 'rumors': 4169, 'smoked': 4170, 'notarized': 4171, 'warranted': 4172, 'curfew': 4173, 'remedy': 4174, 'stunning': 4175, 'pieces': 4176, 'ways': 4177, 'cooled': 4178, 'optional': 4179, 'massacre': 4180, 'convenient': 4181, 'convincing': 4182, 'deliberate': 4183, 'depressing': 4184, 'disastrous': 4185, 'persuasive': 4186, 'unexpected': 4187, 'wobbly': 4188, 'club': 4189, 'dictionary': 4190, 'confidential': 4191, 'questionable': 4192, 'imprecise': 4193, 'starting': 4194, 'unauthorized': 4195, 'onerous': 4196, 'chin': 4197, 'wrestle': 4198, 'hut': 4199, 'straws': 4200, 'peek': 4201, 'ephemeral': 4202, 'conquers': 4203, 'transient': 4204, 'vivacious': 4205, 'hearing': 4206, 'aching': 4207, 'opposites': 4208, 'attract': 4209, 'production': 4210, 'thorns': 4211, 'appeals': 4212, 'figure': 4213, 'attractive': 4214, 'dear': 4215, 'turner': 4216, 'fashionable': 4217, 'hyperactive': 4218, 'turning': 4219, 'golden': 4220, 'lullaby': 4221, 'sloths': 4222, 'somehow': 4223, 'lane': 4224, 'scale': 4225, 'mimicking': 4226, 'termites': 4227, 'depresses': 4228, 'essential': 4229, 'fir': 4230, 'classified': 4231, 'disturbing': 4232, 'exactly': 4233, 'opinion': 4234, 'defense': 4235, 'surprising': 4236, 'spirit': 4237, 'undeniable': 4238, 'handy': 4239, 'barn': 4240, 'cable': 4241, 'snapped': 4242, 'clapped': 4243, 'donkey': 4244, 'brayed': 4245, 'squeaked': 4246, 'ajar': 4247, 'rotates': 4248, 'jammed': 4249, 'dull': 4250, 'lemon': 4251, 'market': 4252, 'motor': 4253, 'meager': 4254, 'shop': 4255, 'system': 4256, 'tactic': 4257, 'verdict': 4258, 'continued': 4259, 'gods': 4260, 'trial': 4261, 'damage': 4262, 'cannibals': 4263, 'murderers': 4264, 'bucket': 4265, 'coconut': 4266, 'extortion': 4267, 'gibberish': 4268, 'insulting': 4269, 'irregular': 4270, 'justified': 4271, 'pointless': 4272, 'priceless': 4273, 'skim': 4274, 'tasteless': 4275, 'temporary': 4276, 'worrisome': 4277, 'silver': 4278, 'thursday': 4279, 'arrives': 4280, 'behaved': 4281, 'despised': 4282, 'appear': 4283, 'flinch': 4284, 'expects': 4285, 'excluded': 4286, 'iq': 4287, 'iphone': 4288, 'arthritis': 4289, 'frostbite': 4290, 'seniority': 4291, 'hasn': 4292, 'juan': 4293, 'bookworm': 4294, 'designer': 4295, 'jeweller': 4296, 'magician': 4297, 'mechanic': 4298, 'pushover': 4299, 'teenager': 4300, 'werewolf': 4301, 'believable': 4302, 'determined': 4303, 'handcuffed': 4304, 'custody': 4305, 'therapy': 4306, 'indecisive': 4307, 'insightful': 4308, 'intolerant': 4309, 'passionate': 4310, 'perspiring': 4311, 'possessive': 4312, 'stuttering': 4313, 'suspicious': 4314, 'unfaithful': 4315, 'minor': 4316, 'trained': 4317, 'mumbles': 4318, 'outlived': 4319, 'served': 4320, 'humbly': 4321, 'sneezes': 4322, 'bait': 4323, 'vomited': 4324, 'updates': 4325, 'fanatic': 4326, 'fireman': 4327, 'negligent': 4328, 'imaginative': 4329, 'acrylic': 4330, 'longer': 4331, 'liquid': 4332, 'meeting': 4333, 'vaccine': 4334, 'visitor': 4335, 'warrant': 4336, 'suppliers': 4337, 'witnesses': 4338, 'parks': 4339, 'canoe': 4340, 'battle': 4341, 'mothers': 4342, 'changing': 4343, 'journalists': 4344, 'pessimistic': 4345, 'pulling': 4346, 'collection': 4347, 'tacky': 4348, 'delay': 4349, 'troubles': 4350, 'achieved': 4351, 'asking': 4352, 'target': 4353, 'arrive': 4354, 'melon': 4355, 'confucius': 4356, 'drama': 4357, 'c': 4358, 'woods': 4359, 'wriggle': 4360, 'actresses': 4361, 'murderer': 4362, 'particular': 4363, 'scaring': 4364, 'telling': 4365, 'unfriendly': 4366, 'scratched': 4367, 'comma': 4368, 'insect': 4369, 'revolt': 4370, 'brewing': 4371, 'demanding': 4372, 'criminals': 4373, 'buddhist': 4374, 'using': 4375, 'attendance': 4376, 'contains': 4377, 'hops': 4378, 'chirping': 4379, 'pillow': 4380, 'rackets': 4381, 'menu': 4382, 'caption': 4383, 'divide': 4384, 'conquer': 4385, 'mozart': 4386, 'snails': 4387, 'copycat': 4388, 'patronize': 4389, 'relaxes': 4390, 'dust': 4391, 'shelf': 4392, 'endorse': 4393, 'tore': 4394, 'portion': 4395, 'breakable': 4396, 'broom': 4397, 'remote': 4398, 'wrench': 4399, 'thanksgiving': 4400, 'od': 4401, 'foolishly': 4402, 'mile': 4403, 'cycling': 4404, 'daydreamer': 4405, 'fishmonger': 4406, 'classmate': 4407, 'heartless': 4408, 'lacks': 4409, 'judgement': 4410, 'retaliate': 4411, 'rolled': 4412, 'chinese': 4413, 'thrust': 4414, 'weighs': 4415, 'kilos': 4416, 'lottery': 4417, 'walker': 4418, 'filthy': 4419, 'critic': 4420, 'ghostwriter': 4421, 'keeper': 4422, 'meth': 4423, 'englishman': 4424, 'overconfident': 4425, 'fifty': 4426, 'scapegoat': 4427, 'torn': 4428, 'sooty': 4429, 'baggage': 4430, 'eludes': 4431, 'pace': 4432, 'quickened': 4433, 'untidy': 4434, 'refill': 4435, 'economy': 4436, 'museum': 4437, 'cope': 4438, 'absolutely': 4439, 'customers': 4440, 'fond': 4441, 'prophet': 4442, 'anticipated': 4443, 'fries': 4444, 'potato': 4445, 'chips': 4446, 'peanuts': 4447, 'clutch': 4448, 'exclude': 4449, 'imagine': 4450, 'clicked': 4451, 'detest': 4452, 'soda': 4453, 'vodka': 4454, 'beets': 4455, 'miserably': 4456, 'tests': 4457, 'fastened': 4458, 'fractured': 4459, 'bureaucracy': 4460, 'most': 4461, 'complaint': 4462, 'nosebleed': 4463, 'telescope': 4464, 'blonde': 4465, 'cabin': 4466, 'frizzy': 4467, 'appetite': 4468, 'siblings': 4469, 'invoice': 4470, 'rehearse': 4471, 'cameras': 4472, 'nephews': 4473, 'hid': 4474, 'somewhere': 4475, 'pans': 4476, 'jog': 4477, 'redecorated': 4478, 'imagination': 4479, 'irish': 4480, 'places': 4481, 'disco': 4482, 'skirts': 4483, 'photography': 4484, 'translating': 4485, 'windsurfing': 4486, 'poverty': 4487, 'bearded': 4488, 'butterflies': 4489, 'fairy': 4490, 'tales': 4491, 'melody': 4492, 'promises': 4493, 'photocopies': 4494, 'mopped': 4495, 'pad': 4496, 'secretary': 4497, 'envelope': 4498, 'lotion': 4499, 'inspiration': 4500, 'nutmeg': 4501, 'doubted': 4502, 'trumpet': 4503, 'explosion': 4504, 'slammed': 4505, 'nine': 4506, 'hours': 4507, 'cigarettes': 4508, 'psychology': 4509, 'ankle': 4510, 'unloaded': 4511, 'unplugged': 4512, 'limp': 4513}\n"
     ]
    }
   ],
   "source": [
    "print(eng_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7edbcc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 1], [27, 1], [27, 1]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db1c55ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 72, 9, 2], [1, 340, 3, 2], [1, 27, 523, 9, 2]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer()\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb258776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 사전 크기 :  4514\n",
      "프랑스어 사전 크기 7263\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print(\"영어 사전 크기 : \", eng_vocab_size)\n",
    "print(\"프랑스어 사전 크기\", fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17cc88f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스 최대길이 :  8\n",
      "프랑스어 시퀀스 최대길이 :  17\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "\n",
    "print(\"영어 시퀀스 최대길이 : \", max_eng_seq_len)\n",
    "print(\"프랑스어 시퀀스 최대길이 : \", max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f13b3633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 :  33000\n",
      "영어 사전 크기 :  4514\n",
      "프랑스어 사전 크기 7263\n",
      "영어 시퀀스 최대길이 :  8\n",
      "프랑스어 시퀀스 최대길이 :  17\n"
     ]
    }
   ],
   "source": [
    "print(\"전체 샘플의 수 : \",  len(lines))\n",
    "print(\"영어 사전 크기 : \", eng_vocab_size)\n",
    "print(\"프랑스어 사전 크기\", fra_vocab_size)\n",
    "print(\"영어 시퀀스 최대길이 : \", max_eng_seq_len)\n",
    "print(\"프랑스어 시퀀스 최대길이 : \", max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f27e4f",
   "metadata": {},
   "source": [
    "디코더 입력의 종료 토큰 제거, 디코더 출력의 시작 토큰 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ed545fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "encoder_input = input_text\n",
    "\n",
    "# 디코더 입력의 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc52c9e",
   "metadata": {},
   "source": [
    "#### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "400e7dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 input 데이터 : (33000, 8)\n",
      "프랑스어 input 데이터 :  (33000, 17)\n",
      "프랑스어 target 데이터 :  (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_fra_seq_len, padding='post')\n",
    "\n",
    "print(\"영어 input 데이터 :\", np.shape(encoder_input))\n",
    "print(\"프랑스어 input 데이터 : \", np.shape(decoder_input))\n",
    "print(\"프랑스어 target 데이터 : \", np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "501b38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 17)\n",
      "(30000, 17)\n",
      "(3000, 8)\n",
      "(3000, 17)\n",
      "(3000, 17)\n"
     ]
    }
   ],
   "source": [
    "# 33000개중 3000개를 validation set으로 둔다.\n",
    "\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]     ## ???? -n_of_val\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc41b0",
   "metadata": {},
   "source": [
    "## step 4. 임베딩층 사용하기\n",
    "## step 5.  모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881ed0a",
   "metadata": {},
   "source": [
    "### Train\n",
    "#### Encoder\n",
    "\n",
    "인코더에서 사용할 임베딩 층 사용 예시\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "\n",
    "enc_emb =  Embedding(단어장의 크기, 임베딩 벡터의 차원)(encoder_inputs)\n",
    "\n",
    "\n",
    "encoder_lstm = LSTM(hidden state의 크기, return_state=True)\n",
    "\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b29ee8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "# encoder embedding\n",
    "enc_emb = Embedding(eng_vocab_size, 256, input_length=max_eng_seq_len)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e3ab2",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4a781ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# decoder embedding\n",
    "dec_emb = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm=LSTM(units=256, return_sequences = True, return_state=True)\n",
    "decoder_outputs,_,_= decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f5556f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1155584     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    1859328     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 7263)   1866591     lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,932,127\n",
      "Trainable params: 5,932,127\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7426cb",
   "metadata": {},
   "source": [
    "label이 integer 값이므로 sparse categorical entropy loss를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbdedfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85f0b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4419 - acc: 0.9362 - val_loss: 1.1686 - val_acc: 0.8324\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4395 - acc: 0.9367 - val_loss: 1.1690 - val_acc: 0.8318\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4376 - acc: 0.9367 - val_loss: 1.1686 - val_acc: 0.8311\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4361 - acc: 0.9368 - val_loss: 1.1764 - val_acc: 0.8311\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4347 - acc: 0.9368 - val_loss: 1.1772 - val_acc: 0.8315\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4330 - acc: 0.9372 - val_loss: 1.1768 - val_acc: 0.8322\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4315 - acc: 0.9372 - val_loss: 1.1808 - val_acc: 0.8315\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4306 - acc: 0.9369 - val_loss: 1.1811 - val_acc: 0.8312\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4298 - acc: 0.9374 - val_loss: 1.1828 - val_acc: 0.8311\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4280 - acc: 0.9373 - val_loss: 1.1891 - val_acc: 0.8310\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4260 - acc: 0.9375 - val_loss: 1.1926 - val_acc: 0.8305\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4248 - acc: 0.9376 - val_loss: 1.1919 - val_acc: 0.8306\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4236 - acc: 0.9377 - val_loss: 1.1980 - val_acc: 0.8293\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4218 - acc: 0.9379 - val_loss: 1.2012 - val_acc: 0.8297\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4212 - acc: 0.9380 - val_loss: 1.2074 - val_acc: 0.8295\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4197 - acc: 0.9378 - val_loss: 1.2103 - val_acc: 0.8303\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4185 - acc: 0.9380 - val_loss: 1.2096 - val_acc: 0.8288\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4176 - acc: 0.9382 - val_loss: 1.2147 - val_acc: 0.8297\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4167 - acc: 0.9382 - val_loss: 1.2222 - val_acc: 0.8293\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4157 - acc: 0.9382 - val_loss: 1.2301 - val_acc: 0.8284\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4150 - acc: 0.9386 - val_loss: 1.2321 - val_acc: 0.8290\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4145 - acc: 0.9384 - val_loss: 1.2404 - val_acc: 0.8285\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4134 - acc: 0.9388 - val_loss: 1.2438 - val_acc: 0.8274\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4128 - acc: 0.9386 - val_loss: 1.2562 - val_acc: 0.8268\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4117 - acc: 0.9388 - val_loss: 1.2643 - val_acc: 0.8268\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4115 - acc: 0.9387 - val_loss: 1.2600 - val_acc: 0.8270\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4110 - acc: 0.9386 - val_loss: 1.2649 - val_acc: 0.8277\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4113 - acc: 0.9388 - val_loss: 1.2739 - val_acc: 0.8258\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4121 - acc: 0.9388 - val_loss: 1.2697 - val_acc: 0.8255\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4119 - acc: 0.9390 - val_loss: 1.2797 - val_acc: 0.8258\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4119 - acc: 0.9389 - val_loss: 1.2843 - val_acc: 0.8258\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4124 - acc: 0.9392 - val_loss: 1.2902 - val_acc: 0.8267\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4124 - acc: 0.9391 - val_loss: 1.2878 - val_acc: 0.8280\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4119 - acc: 0.9391 - val_loss: 1.3055 - val_acc: 0.8267\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4117 - acc: 0.9392 - val_loss: 1.3021 - val_acc: 0.8266\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4125 - acc: 0.9388 - val_loss: 1.2968 - val_acc: 0.8262\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4124 - acc: 0.9388 - val_loss: 1.2996 - val_acc: 0.8254\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4121 - acc: 0.9388 - val_loss: 1.2910 - val_acc: 0.8251\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4115 - acc: 0.9392 - val_loss: 1.2979 - val_acc: 0.8249\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4115 - acc: 0.9389 - val_loss: 1.2998 - val_acc: 0.8257\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4104 - acc: 0.9390 - val_loss: 1.3029 - val_acc: 0.8258\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4094 - acc: 0.9392 - val_loss: 1.2985 - val_acc: 0.8261\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4097 - acc: 0.9391 - val_loss: 1.2943 - val_acc: 0.8251\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4084 - acc: 0.9392 - val_loss: 1.2956 - val_acc: 0.8267\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4080 - acc: 0.9392 - val_loss: 1.2996 - val_acc: 0.8262\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4066 - acc: 0.9393 - val_loss: 1.2928 - val_acc: 0.8247\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4057 - acc: 0.9394 - val_loss: 1.2918 - val_acc: 0.8251\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4053 - acc: 0.9393 - val_loss: 1.3030 - val_acc: 0.8259\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4046 - acc: 0.9396 - val_loss: 1.2978 - val_acc: 0.8263\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.4029 - acc: 0.9396 - val_loss: 1.2977 - val_acc: 0.8253\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "         validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "         batch_size = 32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43321c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fce7cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7734a9",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "##### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d658161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 256)         1155584   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 1,680,896\n",
      "Trainable params: 1,680,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs=encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84380e8",
   "metadata": {},
   "source": [
    "##### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4a80d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ae0f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    1859328     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 256),  525312      embedding_3[0][0]                \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 7263)   1866591     lstm_4[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,251,231\n",
      "Trainable params: 4,251,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a2b9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7a6dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <sos>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1)) \n",
    "    target_seq[0, 0] = fra2idx['<sos>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장     \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18ca3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "342ef0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<sos>']) and i!=fra2idx['<eos>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c657361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: whose kid is that ? \n",
      "정답 문장: qui est cet enfant ? \n",
      "번역기가 번역한 문장:  qui venez venez tie\n",
      "-----------------------------------\n",
      "입력 문장: will you go there ? \n",
      "정답 문장: t y rendras tu ? \n",
      "번역기가 번역한 문장:  tiez tu tiez es \n",
      "-----------------------------------\n",
      "입력 문장: you re surrounded . \n",
      "정답 문장: vous tes cern e . \n",
      "번역기가 번역한 문장:  vous tes tu toi vou\n",
      "-----------------------------------\n",
      "입력 문장: go and wake tom up . \n",
      "정답 문장: allez r veiller tom . \n",
      "번역기가 번역한 문장:  allez tu tu tu t\n",
      "-----------------------------------\n",
      "입력 문장: how much is a room ? \n",
      "정답 문장: combien co te une chambre ? \n",
      "번역기가 번역한 문장:  tiez quel as as pui\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,70,507,1507,1999]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2dd29f",
   "metadata": {},
   "source": [
    "## 회고\n",
    "* 이번 프로젝트에서는 .,소문자 변환, 띄어쓰기, padding등 자연어처리 기법에 있어서 전처리 하는 방법을 알아보았습니다.\n",
    "* word embedding 과 seq2seq 모델의 훈련결과를 알아보았습니다.\n",
    "* 음.... 5개만 번역해보았지만 어느정도 맞는 것도 있었지만 잘 안된 부분도 꽤 있었던 것 같습니다. 학습하는데 시간이 오래걸려서 데이터셋을 33000개로 제한했었지만, 아마 데이터셋의 개수를 더 늘리면 성능이 더 좋아질 것으로 기대됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d380b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
